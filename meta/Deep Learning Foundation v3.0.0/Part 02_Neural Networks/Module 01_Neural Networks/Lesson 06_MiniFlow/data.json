{
  "data": {
    "lesson": {
      "id": 258996,
      "key": "b6deebe4-7f78-4947-b2c6-fc660ca942fb",
      "title": "MiniFlow",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "In this lesson, you'll build your own small version of TensorFlow, called MiniFlow. By building this, you'll gain an understanding of how TensorFlow works under the hood, and gain more insights into important concepts like backpropagation.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/b6deebe4-7f78-4947-b2c6-fc660ca942fb/258996/1544457382287/MiniFlow+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/b6deebe4-7f78-4947-b2c6-fc660ca942fb/258996/1544457380028/MiniFlow+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 259007,
          "key": "d31d9fb9-ad49-411d-8b8f-117c8685240a",
          "title": "Welcome to MiniFlow",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 259008,
              "key": "287d8be4-1415-4e50-bb7c-041e8b636450",
              "title": "Miniflow Introduction",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Nqp_UifEwt0",
                "china_cdn_id": "Nqp_UifEwt0.mp4"
              }
            }
          ]
        },
        {
          "id": 216717,
          "key": "75d0ae66-a676-424f-8fd4-3f0c23154f48",
          "title": "Graphs",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 216718,
              "key": "c6edfbbb-15cb-4dd3-a359-7f5d06540803",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### What is a Neural Network?",
              "instructor_notes": ""
            },
            {
              "id": 216719,
              "key": "7c4be966-9bba-4de7-ad33-6132e18bf862",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/58375218_example-neural-network/example-neural-network.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7c4be966-9bba-4de7-ad33-6132e18bf862",
              "caption": "An example neural network.",
              "alt": null,
              "width": 729,
              "height": 685,
              "instructor_notes": null
            },
            {
              "id": 216720,
              "key": "e4ddf852-4d05-4426-bad7-248480a782fa",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "A neural network is a graph of mathematical functions such as [linear combinations](https://en.wikipedia.org/wiki/Linear_combination) and activation functions. The graph consists of **nodes**, and **edges**.\n\nNodes in each layer (except for nodes in the input layer) perform mathematical functions using inputs from nodes in the previous layers. For example, a node could represent  <span class=\"mathquill\">f(x, y) = x + y</span>, where <span class=\"mathquill\">x</span> and <span class=\"mathquill\">y</span> are input values from nodes in the previous layer. \n\nSimilarly, each node creates an output value which may be passed to nodes in the next layer. The output value from the output layer does not get passed to a future layer (because it is the final layer).\n\nLayers between the input layer and the output layer are called **hidden layers**.\n\nThe edges in the graph describe the connections between the nodes, along which the values flow from one layer to the next. These edges can also apply operations to the values that flow along them, such as multiplying by weights and adding biases. MiniFlow won't use a separate class for edges - instead, its nodes will perform both their own calculations and those of their input edges. This will be more clear as you go through these lessons.\n\n### Forward Propagation\n\nBy propagating values from the first layer (the input layer) through all the mathematical functions represented by each node, the network outputs a value. This process is called a **forward pass**.\n\nHere's an example of a simple forward pass.\n\n<video width=\"100%\" controls loop >\n  <source src=\"https://s3.amazonaws.com/content.udacity-data.com/courses/carnd/videos/input-to-output-2.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag. <a href=\"https://s3.amazonaws.com/content.udacity-data.com/courses/carnd/videos/input-to-output-2.mp4\" target=\"_blank\">Click here to see the animation.</a>\n</video>\n\nNotice that the output layer performs a mathematical function, addition, on its inputs. There is no hidden layer.\n\n### Graphs\n\nThe nodes and edges create a graph structure. Though the example above is fairly simple, it isn't hard to imagine that increasingly complex graphs can calculate . . . well . . . *almost anything*.\n\nThere are generally two steps to create neural networks:\n\n1. Define the graph of nodes and edges.\n2. Propagate values through the graph.\n\n`MiniFlow` works the same way. You'll define the nodes and edges of your network with one method and then propagate values through the graph with another method. `MiniFlow` comes with some starter code to help you out. We'll take a look on the next page, but first, let's test your intuition.",
              "instructor_notes": ""
            },
            {
              "id": 216733,
              "key": "6d01216a-5654-428e-868c-3f088785e475",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Graph Quiz",
              "instructor_notes": ""
            },
            {
              "id": 216732,
              "key": "f6ba59fe-fd84-4f25-a5e2-26290333b668",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/58375a5a_addition-graph/addition-graph.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f6ba59fe-fd84-4f25-a5e2-26290333b668",
              "caption": "",
              "alt": null,
              "width": 297,
              "height": 290,
              "instructor_notes": null
            },
            {
              "id": 216731,
              "key": "93a0c867-956c-4f8a-aaea-8409624c8a0e",
              "title": "Graph Quiz",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "question": {
                "prompt": "In the graph above, what will the value of the addition node be?",
                "answers": [
                  {
                    "id": "a1480023740298",
                    "text": "4",
                    "is_correct": false
                  },
                  {
                    "id": "a1480023786650",
                    "text": "10",
                    "is_correct": false
                  },
                  {
                    "id": "a1480024067161",
                    "text": "14",
                    "is_correct": true
                  },
                  {
                    "id": "a1480024069870",
                    "text": "0",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 216721,
          "key": "891a6f78-e344-47e6-abd8-11867745be76",
          "title": "MiniFlow Architecture",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 216722,
              "key": "440e2459-9523-4a27-9bd7-24255701e21a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# MiniFlow Architecture\n\nLet's consider how to implement this graph structure in `MiniFlow`. We'll use a Python class to represent a generic node.\n\n```\nclass Node(object):\n    def __init__(self):\n        # Properties will go here!\n```\n\nWe know that each node might receive input from multiple other nodes. We also know that each node creates a single output, which will likely be passed to other nodes. Let's add two lists: one to store references to the inbound nodes, and the other to store references to the outbound nodes.\n\n```\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Node(s) from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Node(s) to which this Node passes values\n        self.outbound_nodes = []\n        # For each inbound_node, add the current Node as an outbound_node.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n```\n\nEach node will eventually calculate a value that represents its output. Let's initialize the `value` to `None` to indicate that it exists but hasn't been set yet.\n\n```\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Node(s) from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Node(s) to which this Node passes values\n        self.outbound_nodes = []\n        # For each inbound_node, add the current Node as an outbound_node.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n        # A calculated value\n        self.value = None\n```\n\nEach node will need to be able to pass values forward and perform backpropagation (more on that later). For now, let's add a placeholder method for forward propagation. We'll deal with backpropagation later on.\n\n```\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Node(s) from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Node(s) to which this Node passes values\n        self.outbound_nodes = []\n        # For each inbound_node, add the current Node as an outbound_node.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n        # A calculated value\n        self.value = None\n\n    def forward(self):\n        \"\"\"\n        Forward propagation.\n\n        Compute the output value based on `inbound_nodes` and\n        store the result in self.value.\n        \"\"\"\n        raise NotImplemented\n```\n\n### Nodes that Calculate\n\nWhile `Node` defines the base set of properties that every node holds, only specialized [subclasses](https://docs.python.org/3/tutorial/classes.html#inheritance) of `Node` will end up in the graph. As part of this lab, you'll build the subclasses of `Node` that can perform calculations and hold values. For example, consider the `Input` subclass of `Node`.\n\n```\nclass Input(Node):\n    def __init__(self):\n        # An Input node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator.\n        Node.__init__(self)\n\n    # NOTE: Input node is the only node where the value\n    # may be passed as an argument to forward().\n    #\n    # All other node implementations should get the value\n    # of the previous node from self.inbound_nodes\n    #\n    # Example:\n    # val0 = self.inbound_nodes[0].value\n    def forward(self, value=None):\n        # Overwrite the value if one is passed in.\n        if value is not None:\n            self.value = value\n```\n\nUnlike the other subclasses of `Node`,  the `Input` subclass does not actually calculate anything. The `Input` subclass just holds a `value`, such as a data feature or a model parameter (weight/bias). \n\nYou can set `value` either explicitly or with the `forward()` method. This value is then fed through the rest of the neural network.\n\n### The Add Subclass\n\n`Add`, which is another subclass of `Node`, actually can perform a calculation (addition).\n\n```\nclass Add(Node):\n    def __init__(self, x, y):\n        Node.__init__(self, [x, y])\n\n    def forward(self):\n        \"\"\"\n        You'll be writing code here in the next quiz!\n        \"\"\"\n```\n\nNotice the difference in the `__init__` method, `Add.__init__(self, [x, y])`. Unlike the `Input` class, which has no inbound nodes, the `Add` class takes 2 inbound nodes, `x` and `y`, and adds the values of those nodes.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 216723,
          "key": "48bad937-5bfd-4e38-9e61-337ff69f9fb7",
          "title": "Forward Propagation",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 216724,
              "key": "ae4fc343-adce-41d6-9cb7-2efb57366e83",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Forward propagation\n\n`MiniFlow` has two methods to help you define and then run values through your graphs: `topological_sort()` and `forward_pass()`.\n\nIn order to define your network, you'll need to define the order of operations for your nodes. Given that the input to some node depends on the outputs of others, you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting).",
              "instructor_notes": ""
            },
            {
              "id": 216725,
              "key": "c44fda83-d887-4d00-a00b-1fea172ad32a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/581037bb_topological-sort.001/topological-sort.001.jpeg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c44fda83-d887-4d00-a00b-1fea172ad32a",
              "caption": "An example of topological sorting",
              "alt": "",
              "width": 1024,
              "height": 768,
              "instructor_notes": null
            },
            {
              "id": 216726,
              "key": "9cc33292-e3ac-48db-bc1f-9cfa1c6759d0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The `topological_sort()` function implements topological sorting using [Kahn's Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm).  The details of this method are not important, the result is; `topological_sort()` returns a sorted list of nodes in which all of the calculations can run in series. `topological_sort()` takes in a `feed_dict`, which is how we initially set a value for an `Input` node. The `feed_dict` is represented by the Python dictionary data structure. Here's an example use case:\n\n```\n# Define 2 `Input` nodes.\nx, y = Input(), Input()\n\n# Define an `Add` node, the two above `Input` nodes being the input.\nadd = Add(x, y)\n\n# The value of `x` and `y` will be set to 10 and 20 respectively.\nfeed_dict = {x: 10, y: 20}\n\n# Sort the nodes with topological sort.\nsorted_nodes = topological_sort(feed_dict=feed_dict)\n```\n\n(You can find the source code for `topological_sort()` in miniflow.py in the programming quiz below.)\n\nThe other method at your disposal is `forward_pass()`, which actually runs the network and outputs a value.\n\n```\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted nodes.\n\n    Arguments:\n\n        `output_node`: The output node of the graph (no outgoing edges).\n        `sorted_nodes`: a topologically sorted list of nodes.\n\n    Returns the output node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value\n```\n\n### Quiz 1 - Passing Values Forward\n\nCreate and run this graph!",
              "instructor_notes": ""
            },
            {
              "id": 216727,
              "key": "a0be470a-9ba6-4f98-b657-d6d855407d8b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/58375a5a_addition-graph/addition-graph.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a0be470a-9ba6-4f98-b657-d6d855407d8b",
              "caption": "The graph you'll run in this quiz. The node values may change, though!",
              "alt": null,
              "width": 593,
              "height": 580,
              "instructor_notes": null
            },
            {
              "id": 216728,
              "key": "76b46a0a-c80e-43d6-a562-da57c7f4ab39",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Setup\n\nReview `nn.py` and `miniflow.py`.\n\nThe neural network architecture is already there for you in nn.py. It's your job to finish `MiniFlow` to make it work.\n\nFor this quiz, I want you to:\n\n1. Open `nn.py` below. **You don't need to change anything.** I just want you to see how `MiniFlow` works.\n2. Open `miniflow.py`. **Finish the `forward` method on the `Add` class. All that's required to pass this quiz is a correct implementation of `forward`.**\n3. Test your network by hitting \"Test Run!\" When the output looks right, hit \"Submit!\"\n\n(You'll find the solution on the next page.)",
              "instructor_notes": ""
            },
            {
              "id": 216729,
              "key": "cfd01a0a-56cc-4282-b116-eb0437adb774",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "4877223226114048",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nThis script builds and runs a graph with miniflow.\n\nThere is no need to change anything to solve this quiz!\n\nHowever, feel free to play with the network! Can you also\nbuild a network that solves the equation below?\n\n(x + y) + y\n\"\"\"\n\nfrom miniflow import *\n\nx, y = Input(), Input()\n\nf = Add(x, y)\n\nfeed_dict = {x: 10, y: 5}\n\nsorted_nodes = topological_sort(feed_dict)\noutput = forward_pass(f, sorted_nodes)\n\n# NOTE: because topological_sort sets the values for the `Input` nodes we could also access\n# the value for x with x.value (same goes for y).\nprint(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nYou need to change the Add() class below.\n\"\"\"\n\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Nodes from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Nodes to which this Node passes values\n        self.outbound_nodes = []\n        # A calculated value\n        self.value = None\n        # Add this node as an outbound node on its inputs.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n\n    # These will be implemented in a subclass.\n    def forward(self):\n        \"\"\"\n        Forward propagation.\n\n        Compute the output value based on `inbound_nodes` and\n        store the result in self.value.\n        \"\"\"\n        raise NotImplemented\n\n\nclass Input(Node):\n    def __init__(self):\n        # an Input node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator\n        Node.__init__(self)\n\n    # NOTE: Input node is the only node that may\n    # receive its value as an argument to forward().\n    #\n    # All other node implementations should calculate their\n    # values from the value of previous nodes, using\n    # self.inbound_nodes\n    #\n    # Example:\n    # val0 = self.inbound_nodes[0].value\n    def forward(self, value=None):\n        if value is not None:\n            self.value = value\n\n\nclass Add(Node):\n    def __init__(self, x, y):\n        # You could access `x` and `y` in forward with\n        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n        Node.__init__(self, [x, y])\n\n    def forward(self):\n        \"\"\"\n        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n        Remember to grab the value of each inbound_node to sum!\n\n        Your code here!\n        \"\"\"\n\n\n\"\"\"\nNo need to change anything below here!\n\"\"\"\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort generic nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted nodes.\n\n    Arguments:\n\n        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n        `sorted_nodes`: A topologically sorted list of nodes.\n\n    Returns the output Node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 196567,
          "key": "9f4a9317-ce3b-4cf0-93c8-68080d2742f3",
          "title": "Forward Propagation Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 196568,
              "key": "d9c4b118-b206-44c6-8efb-b01ca4df88a7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's my solution (I'm just showing `forward` method of the `Add` class):\n\n```\ndef forward(self):\n    x_value = self.inbound_nodes[0].value\n    y_value = self.inbound_nodes[1].value\n    self.value = x_value + y_value\n```\n\nWhile this looks simple, I want to show you why I used `x_value` and `y_value` from the `inbound_nodes` array. Let's take a look at the start with `Node`'s constructor:\n\n```\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Node(s) from which this Node receives values.\n        self.inbound_nodes = inbound_nodes\n        # Removed everything else for brevity.\n```\n\n`inbound_nodes` are set when the `Node` is instantiated. \n\nOf course, you weren't using `Node` directly, rather you used `Add`, which is a subclass of `Node`. `Add`'s constructor is responsible for passing the `inbound_nodes` to `Node`, which happens here:\n\n```\nclass Add(Node):\n    def __init__(self, x, y):\n         Node.__init__(self, [x, y]) # calls Node's constructor\n    ...\n```\n\nLastly, there's the question of why `node.value` holds the value of the inputs.  For each node of the `Input()` class, the nodes are set directly when you run `topological_sort`:\n```\ndef topological_sort(feed_dict):\n    ...\n    if isinstance(n, Input):\n        n.value = feed_dict[n]\n    ...\n```\nFor other classes, the value of node.value is set in the forward pass:\n```\ndef forward_pass(output_node, sorted_nodes):\n    ...\n    for n in sorted_nodes:\n        n.forward()\n    ...\n```\n\nAnd that's it for addition! \n\nKeep going to make `MiniFlow` more capable.",
              "instructor_notes": ""
            },
            {
              "id": 196742,
              "key": "6d08a933-d2f4-426d-a050-8385fd65f4d4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Bonus Challenges!\n\nThese are **ungraded** challenges as they are more of a test of your Python skills than neural network skills.\n\n1. Can you make `Add` accept any number of inputs? Eg. `Add(x, y, z)`.\n2. Can you make a `Mul` class that multiplies *n* inputs?",
              "instructor_notes": ""
            },
            {
              "id": 196743,
              "key": "8ae9ef50-2044-42b3-a850-0c3e8c254751",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5487356826222592",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nNo need to change anything here!\n\nIf all goes well, this should work after you\nmodify the Add class in miniflow.py.\n\"\"\"\n\nfrom miniflow import *\n\nx, y, z = Input(), Input(), Input()\n\nf = Add(x, y, z)\n\nfeed_dict = {x: 4, y: 5, z: 10}\n\ngraph = topological_sort(feed_dict)\noutput = forward_pass(f, graph)\n\n# should output 19\nprint(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nBonus Challenge!\n\nWrite your code in Add (scroll down).\n\"\"\"\n\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        # Nodes from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Nodes to which this Node passes values\n        self.outbound_nodes = []\n        # A calculated value\n        self.value = None\n        # Add this node as an outbound node on its inputs.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n\n    # These will be implemented in a subclass.\n    def forward(self):\n        \"\"\"\n        Forward propagation.\n\n        Compute the output value based on `inbound_nodes` and\n        store the result in self.value.\n        \"\"\"\n        raise NotImplemented\n\n\nclass Input(Node):\n    def __init__(self):\n        # An Input Node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator\n        Node.__init__(self)\n\n    # NOTE: Input Node is the only Node where the value\n    # may be passed as an argument to forward().\n    #\n    # All other Node implementations should get the value\n    # of the previous nodes from self.inbound_nodes\n    #\n    # Example:\n    # val0 = self.inbound_nodes[0].value\n    def forward(self, value=None):\n        # Overwrite the value if one is passed in.\n        if value is not None:\n            self.value = value\n\n\n\"\"\"\nCan you augment the Add class so that it accepts\nany number of nodes as input?\n\nHint: this may be useful:\nhttps://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n\"\"\"\nclass Add(Node):\n    # You may need to change this...\n    def __init__(self, *inputs):\n        Node.__init__(self, inputs)\n\n    def forward(self):\n        \"\"\"\n        For reference, here's the old way from the last\n        quiz. You'll want to write code here.\n        \"\"\"\n        # x_value = self.inbound_nodes[0].value\n        # y_value = self.inbound_nodes[1].value\n        # self.value = x_value + y_value\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted nodes.\n\n    Arguments:\n\n        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n        `sorted_nodes`: A topologically sorted list of nodes.\n\n    Returns the output Node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 262842,
          "key": "96656ffd-8c94-43f4-a962-8d44319f8c9f",
          "title": "Learning and Loss",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 262843,
              "key": "d40586a5-ab49-46ea-84d5-20e18cc082b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Learning and Loss\n\nLike `MiniFlow` in its current state, neural networks take inputs and produce outputs. But unlike `MiniFlow` in its current state, neural networks can *improve* the accuracy of their outputs over time (it's hard to imagine improving the accuracy of `Add` over time!). To explore why accuracy matters, I want you to first implement a trickier (and more useful!) node than `Add`: the `Linear` node.\n\n### The Linear Function\n",
              "instructor_notes": ""
            },
            {
              "id": 262849,
              "key": "5695ba65-af41-4c18-bc43-897e6166d129",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/5892978b_neuron/neuron.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5695ba65-af41-4c18-bc43-897e6166d129",
              "caption": "",
              "alt": "",
              "width": 974,
              "height": 540,
              "instructor_notes": null
            },
            {
              "id": 262844,
              "key": "69ecec35-f785-48dc-98f1-343193b67369",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nThink back to the Intro to Neural Networks lesson. A simple artificial neuron depends on three components:\n\n* inputs, <span class=\"mathquill\"> x_i</span>\n* weights, <span class=\"mathquill\"> w_i</span> \n* bias,  <span class=\"mathquill\"> b </span>\n\nThe output,  <span class=\"mathquill\">y</span>, is just the weighted sum of the inputs plus the bias.",
              "instructor_notes": ""
            },
            {
              "id": 262846,
              "key": "2eeeb496-b2eb-405e-a2ce-0593b697ed20",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Remember, by varying the weights, you can vary the amount of influence any given input has on the output. The learning aspect of neural networks takes place during a process known as backpropagation. In backpropogation, the network modifies the weights to improve the network's output accuracy. You'll be applying all of this shortly.\n\nIn this next quiz, you'll try to build a linear neuron that generates an output by applying a simplified version of the weighted sum. `Linear` should take a list of inbound nodes of length *n*, a list of weights of length *n*, and a bias.\n\n### Instructions\n\n1. Open nn.py below. Read through the neural network to see the expected output of `Linear`.\n2. Open miniflow.py below. Modify `Linear`, which is a subclass of `Node`, to generate an output with  <span class=\"mathquill\"> y = \\sum w_i x_i + b</span>.\n\n(Hint: you could use `numpy` to solve this quiz if you'd like, but it's possible to solve this with plain Python.)",
              "instructor_notes": ""
            },
            {
              "id": 262847,
              "key": "7048729a-835a-4129-90f1-f8b07577b3f7",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5576058842775552",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nNOTE: Here we're using an Input node for more than a scalar.\nIn the case of weights and inputs the value of the Input node is\nactually a python list!\n\nIn general, there's no restriction on the values that can be passed to an Input node.\n\"\"\"\nfrom miniflow import *\n\ninputs, weights, bias = Input(), Input(), Input()\n\nf = Linear(inputs, weights, bias)\n\nfeed_dict = {\n    inputs: [6, 14, 3],\n    weights: [0.5, 0.25, 1.4],\n    bias: 2\n}\n\ngraph = topological_sort(feed_dict)\noutput = forward_pass(f, graph)\n\nprint(output) # should be 12.7 with this example",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nWrite the Linear#forward method below!\n\"\"\"\n\n\nclass Node:\n    def __init__(self, inbound_nodes=[]):\n        # Nodes from which this Node receives values\n        self.inbound_nodes = inbound_nodes\n        # Nodes to which this Node passes values\n        self.outbound_nodes = []\n        # A calculated value\n        self.value = None\n        # Add this node as an outbound node on its inputs.\n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n\n    # These will be implemented in a subclass.\n    def forward(self):\n        \"\"\"\n        Forward propagation.\n\n        Compute the output value based on `inbound_nodes` and\n        store the result in self.value.\n        \"\"\"\n        raise NotImplemented\n\n\nclass Input(Node):\n    def __init__(self):\n        # An Input Node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator\n        Node.__init__(self)\n\n        # NOTE: Input Node is the only Node where the value\n        # may be passed as an argument to forward().\n        #\n        # All other Node implementations should get the value\n        # of the previous nodes from self.inbound_nodes\n        #\n        # Example:\n        # val0 = self.inbound_nodes[0].value\n    def forward(self, value=None):\n        # Overwrite the value if one is passed in.\n        if value is not None:\n            self.value = value\n\n\nclass Linear(Node):\n    def __init__(self, inputs, weights, bias):\n        Node.__init__(self, [inputs, weights, bias])\n\n        # NOTE: The weights and bias properties here are not\n        # numbers, but rather references to other nodes.\n        # The weight and bias values are stored within the\n        # respective nodes.\n\n    def forward(self):\n        \"\"\"\n        Set self.value to the value of the linear function output.\n\n        Your code goes here!\n        \"\"\"\n        pass\n\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted nodes.\n\n    Arguments:\n\n        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n        `sorted_nodes`: A topologically sorted list of nodes.\n\n    Returns the output Node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 262875,
          "key": "2e5f4127-f226-4572-bbe6-8196e7dfabd5",
          "title": "Linear Transform",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 262876,
              "key": "8b73a532-3f67-4157-be6b-5f00fd35a179",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "#### Solution to Linear Node\n\nHere's my solution to the last quiz:\n\n```\nclass Linear(Node):\n    def __init__(self, inputs, weights, bias):\n        Node.__init__(self, [inputs, weights, bias])\n\n    def forward(self):\n        \"\"\"\n        Set self.value to the value of the linear function output.\n\n        Your code goes here!\n        \"\"\"\n        inputs = self.inbound_nodes[0].value\n        weights = self.inbound_nodes[1].value\n        bias = self.inbound_nodes[2]\n        self.value = bias.value\n        for x, w in zip(inputs, weights):\n            self.value += x * w\n\n```\n\nIn the solution, I set `self.value` to the bias and then loop through the inputs and weights, adding each weighted input to `self.value`. Notice calling `.value` on `self.inbound_nodes[0]` or `self.inbound_nodes[1]` gives us a list.",
              "instructor_notes": ""
            },
            {
              "id": 262877,
              "key": "fac5b640-024b-4e83-8826-a49d405668ae",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/5892a358_screen-shot-2016-10-21-at-15.43.05/screen-shot-2016-10-21-at-15.43.05.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fac5b640-024b-4e83-8826-a49d405668ae",
              "caption": "Shift your thinking here to the edges between layers.",
              "alt": null,
              "width": 916,
              "height": 1406,
              "instructor_notes": null
            },
            {
              "id": 262878,
              "key": "f414b5cd-8a69-4be4-a6a2-cb328c875240",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "[Linear algebra](https://www.khanacademy.org/math/linear-algebra) nicely reflects the idea of transforming values between layers in a graph. In fact, the concept of a [transform](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/v/vector-transformations) does exactly what a layer should do - it converts inputs to outputs in many dimensions.\n\nLet's go back to our equation for the output.",
              "instructor_notes": ""
            },
            {
              "id": 262899,
              "key": "07db3313-8dfc-427c-8c0a-c1aa6e4d62e4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/5892a66c_neuron-output/neuron-output.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/07db3313-8dfc-427c-8c0a-c1aa6e4d62e4",
              "caption": "Equation (1)",
              "alt": null,
              "width": 150,
              "height": 45,
              "instructor_notes": null
            },
            {
              "id": 262879,
              "key": "c0798588-d081-4b67-a6f0-0ce6a7e87055",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For the rest of this section we'll denote *x* as *X* and *w* as *W* since they are now matrices, and *b* is now a vector instead of a scalar.\n\nConsider a `Linear` node with 1 input and k outputs (mapping 1 input to k outputs). In this context an input/output is synonymous with a feature.\n\nIn this case *X* is a 1 by 1 matrix.",
              "instructor_notes": ""
            },
            {
              "id": 262880,
              "key": "261df8f0-6d07-442f-a4b7-aaf377fd03ca",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/581f9571_newx/newx.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/261df8f0-6d07-442f-a4b7-aaf377fd03ca",
              "caption": "1 by 1 matrix, 1 element.",
              "alt": null,
              "width": 110,
              "height": 30,
              "instructor_notes": null
            },
            {
              "id": 262881,
              "key": "b60a6058-aba3-46bf-a8a1-b9d5a64350ee",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "*W* becomes a 1 by k matrix (looks like a row).",
              "instructor_notes": ""
            },
            {
              "id": 262882,
              "key": "e07a4e2d-09c9-46a4-bd18-1cfc585a27cc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/581f9571_neww/neww.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e07a4e2d-09c9-46a4-bd18-1cfc585a27cc",
              "caption": "A 1 by k weights row matrix.",
              "alt": null,
              "width": 354,
              "height": 30,
              "instructor_notes": null
            },
            {
              "id": 262883,
              "key": "61c31c4b-4834-4a51-be72-ad0241c58e36",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The result of the matrix multiplication of *X* and *W* is a 1 by k matrix. Since *b* is also a 1 by k row matrix (1 bias per output), *b* is added to the output of the *X* and *W* matrix multiplication.\n\nWhat if we are mapping n inputs to k outputs? \n\nThen *X* is now a 1 by n matrix and *W* is a n by k matrix. The result of the matrix multiplication is still a 1 by k matrix so the use of the biases remain the same.",
              "instructor_notes": ""
            },
            {
              "id": 262884,
              "key": "be10983c-e82d-402f-8d26-3b619d523f5e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/581f9570_newx-1n/newx-1n.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/be10983c-e82d-402f-8d26-3b619d523f5e",
              "caption": "X is now a 1 by n matrix, n inputs/features.",
              "alt": null,
              "width": 340,
              "height": 30,
              "instructor_notes": null
            },
            {
              "id": 264591,
              "key": "ba555b85-5824-4dbc-bf5d-eed25861979a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/58a24e51_neww-nk-fixed/neww-nk-fixed.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ba555b85-5824-4dbc-bf5d-eed25861979a",
              "caption": "",
              "alt": null,
              "width": 375,
              "height": 165,
              "instructor_notes": null
            },
            {
              "id": 262886,
              "key": "bc746725-af47-425a-9da8-3f37f57ade79",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/581a94e5_b-1byk/b-1byk.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/bc746725-af47-425a-9da8-3f37f57ade79",
              "caption": "Row matrix of biases, one for each output.",
              "alt": null,
              "width": 247,
              "height": 30,
              "instructor_notes": null
            },
            {
              "id": 262887,
              "key": "d98a3f7d-20ad-4d53-b432-4fbf1269eab1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's take a look at an example of n inputs. Consider an 28px by 28px greyscale image, as is in the case of images in the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a set of handwritten digits. We can reshape the image such that it's a 1 by 784 matrix, n = 784.  Each pixel is an input/feature. Here's an animated example emphasizing a pixel is a feature.",
              "instructor_notes": ""
            },
            {
              "id": 262889,
              "key": "e929acd9-5483-4c09-a8f1-315610b64d07",
              "title": "Pixels are Features!",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "qE5YYXtPq9U",
                "china_cdn_id": "qE5YYXtPq9U.mp4"
              }
            },
            {
              "id": 262890,
              "key": "52132ae2-3cd8-4980-bb26-550f160695ff",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In practice, it's common to feed in multiple data examples in each forward pass rather than just 1. The reasoning for this is the examples can be processed in parallel, resulting in big performance gains. The number of examples is called the *batch size*. Common numbers for the batch size are 32, 64, 128, 256, 512. Generally, it's the most we can comfortably fit in memory.\n\nWhat does this mean for *X*, *W* and *b*?\n\n*X* becomes a m by n matrix and *W* and *b* remain the same. The result of the matrix multiplication is now m by k, so the addition of *b* is [broadcast](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) over each row.",
              "instructor_notes": ""
            },
            {
              "id": 262891,
              "key": "93cd7f25-47f8-4754-80ab-824f7cd4e792",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/5820bdff_x-mn/x-mn.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/93cd7f25-47f8-4754-80ab-824f7cd4e792",
              "caption": "X is now an m by n matrix. Each row has n inputs/features.",
              "alt": null,
              "width": 376,
              "height": 165,
              "instructor_notes": null
            },
            {
              "id": 262894,
              "key": "5eec3283-65b5-4648-b42b-428304d07421",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the context of MNIST each row  of *X* is an image reshaped from 28 by 28 to 1 by 784.",
              "instructor_notes": ""
            },
            {
              "id": 262898,
              "key": "1eb45fe6-71a1-437a-86ef-c55ae5e6d60c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/5892a5ea_z/z.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/1eb45fe6-71a1-437a-86ef-c55ae5e6d60c",
              "caption": "Equation (2)",
              "alt": null,
              "width": 140,
              "height": 20,
              "instructor_notes": null
            },
            {
              "id": 262902,
              "key": "e2f3ad08-c1d3-40be-b8a3-298ee8bf84d7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Equation (2) can also be viewed as *Z = XW + B* where *B* is the biases vector, *b*, stacked m times as a row. Due to broadcasting it's abbreviated to *Z = XW + b*.",
              "instructor_notes": ""
            },
            {
              "id": 262903,
              "key": "9b672173-c4be-4e58-8045-50951e04907a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "I want you to rebuild `Linear` to handle matrices and vectors using the venerable Python math package `numpy` to make your life easier. `numpy` is often abbreviated as `np`, so we'll refer to it as `np` when referring to code. \n\nI used `np.array` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)) to create the matrices and vectors. You'll want to use `np.dot`, which functions as matrix multiplication for 2D arrays ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)), to multiply the input and weights matrices from Equation (2). It's also worth noting that numpy actually overloads the `__add__` operator so you can use it directly with `np.array` (eg. `np.array() + np.array()`).\n\n### Instructions\n\n1. Open nn.py. See how the neural network implements the `Linear` node.\n2. Open miniflow.py. Implement Equation (2) within the forward pass for the `Linear` node.\n3. Test your work!",
              "instructor_notes": ""
            },
            {
              "id": 262904,
              "key": "d771bd9e-7715-4303-ad62-86494346ad8d",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6071508419084288",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nThe setup is similar to the prevous `Linear` node you wrote\nexcept you're now using NumPy arrays instead of python lists.\n\nUpdate the Linear class in miniflow.py to work with\nnumpy vectors (arrays) and matrices.\n\nTest your code here!\n\"\"\"\n\nimport numpy as np\nfrom miniflow import *\n\nX, W, b = Input(), Input(), Input()\n\nf = Linear(X, W, b)\n\nX_ = np.array([[-1., -2.], [-1, -2]])\nW_ = np.array([[2., -3], [2., -3]])\nb_ = np.array([-3., -5])\n\nfeed_dict = {X: X_, W: W_, b: b_}\n\ngraph = topological_sort(feed_dict)\noutput = forward_pass(f, graph)\n\n\"\"\"\nOutput should be:\n[[-9., 4.],\n[-9., 4.]]\n\"\"\"\nprint(output)",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nModify Linear#forward so that it linearly transforms\ninput matrices, weights matrices and a bias vector to\nan output.\n\"\"\"\n\nimport numpy as np\n\n\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        self.inbound_nodes = inbound_nodes\n        self.value = None\n        self.outbound_nodes = []\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward():\n        raise NotImplementedError\n\n\nclass Input(Node):\n    \"\"\"\n    While it may be strange to consider an input a node when\n    an input is only an individual node in a node, for the sake\n    of simpler code we'll still use Node as the base class.\n\n    Think of Input as collating many individual input nodes into\n    a Node.\n    \"\"\"\n    def __init__(self):\n        # An Input node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n\nclass Linear(Node):\n    def __init__(self, X, W, b):\n        # Notice the ordering of the input nodes passed to the\n        # Node constructor.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        \"\"\"\n        Set the value of this node to the linear transform output.\n\n        Your code goes here!\n        \"\"\"\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n        `sorted_nodes`: a topologically sorted list of nodes.\n\n    Returns the output node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 197323,
          "key": "9f421c5e-749c-4de5-ad0c-3812eb34d05d",
          "title": "Sigmoid Function",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 197545,
              "key": "b4f8a35b-436c-4780-ab79-3b04a5717089",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's my solution to the last quiz:\n\n```\nclass Linear(Node):\n    def __init__(self, X, W, b):\n        # Notice the ordering of the inputs passed to the\n        # Node constructor.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n```\n\nNothing fancy in my solution. I pulled the value of the `X`, `W` and `b` from their respective inputs. I used `np.dot` to handle the matrix multiplication.",
              "instructor_notes": ""
            },
            {
              "id": 197324,
              "key": "8e64f630-a5ec-4aee-9b13-bc03a4cdeae8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Sigmoid Function\n\nNeural networks take advantage of alternating transforms and activation functions to better categorize outputs. The sigmoid function is among the most common activation functions.",
              "instructor_notes": ""
            },
            {
              "id": 197842,
              "key": "50dd5bac-7cfb-4272-979a-1d34504e302b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/58102c91_19/19.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/50dd5bac-7cfb-4272-979a-1d34504e302b",
              "caption": "Equation (3)",
              "alt": null,
              "width": 200,
              "height": 44,
              "instructor_notes": null
            },
            {
              "id": 198011,
              "key": "8d13631c-aa8b-48b3-81a5-06065c2dbc97",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/581142a9_pasted-image-at-2016-10-25-01-17-pm/pasted-image-at-2016-10-25-01-17-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8d13631c-aa8b-48b3-81a5-06065c2dbc97",
              "caption": "Graph of the sigmoid function. Notice the \"S\" shape.",
              "alt": null,
              "width": 1300,
              "height": 808,
              "instructor_notes": null
            },
            {
              "id": 197338,
              "key": "d38aa182-d2fd-4fc3-86fd-b39c4c69b30e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Linear transforms are great for simply *shifting* values, but neural networks often require a more nuanced transform. For instance, one of the original designs for an artificial neuron, [the perceptron](https://en.wikipedia.org/wiki/Perceptron), exhibits binary output behavior. Perceptrons compare a weighted input to a threshold. When the weighted input exceeds the threshold, the perceptron is **activated** and outputs `1`, otherwise it outputs `0`.\n\nYou could model a perceptron's behavior as a step function:",
              "instructor_notes": ""
            },
            {
              "id": 198017,
              "key": "6d57007d-6e8a-4388-8fa1-20e7f932ff87",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/581142e2_save-2/save-2.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6d57007d-6e8a-4388-8fa1-20e7f932ff87",
              "caption": "Example of a step function (The jump between y = 0 and y = 1 should be instantaneous).",
              "alt": null,
              "width": 650,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 198013,
              "key": "9b0398fa-af4d-4cd5-8309-97e56c45917c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Activation, the idea of binary output behavior, generally makes sense for classification problems. For example, if you ask the network to hypothesize if a handwritten image is a '9', you're effectively asking for a binary output - *yes*, this is a '9', or *no*, this is not a '9'. A step function is the starkest form of a binary output, which is great, but step functions are not continuous and not differentiable, which is *very bad*. Differentiation is what makes gradient descent possible.\n\nThe sigmoid function, Equation (3) above, replaces thresholding with a beautiful S-shaped curve (also shown above) that mimics the activation behavior of a perceptron while being differentiable. As a bonus, the sigmoid function has a very simple derivative that that can be calculated from the sigmoid function itself, as shown in Equation (4) below.",
              "instructor_notes": ""
            },
            {
              "id": 198018,
              "key": "dd970cc9-c73b-4539-b45b-a2d296c3894a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/58102459_21/21.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/dd970cc9-c73b-4539-b45b-a2d296c3894a",
              "caption": "Equation (4). <span class='mathquill'>\\large \\sigma</span> represents Equation (3), the sigmoid function.",
              "alt": "",
              "width": 220,
              "height": 22,
              "instructor_notes": null
            },
            {
              "id": 197339,
              "key": "e8496e46-70fe-402f-83f6-33155984f883",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice that the sigmoid function only has one parameter. Remember that sigmoid is an *activation* function (*non-linearity*), meaning it takes a single input and performs a mathematical operation on it.\n\nConceptually, the sigmoid function makes decisions. When given weighted features from some data, it indicates whether or not the features contribute to a classification. In that way, a sigmoid activation works well following a linear transformation. As it stands right now with random weights and bias, the sigmoid node's output is also random. The process of learning through backpropagation and gradient descent, which you will implement soon, modifies the weights and bias such that activation of the sigmoid node begins to match expected outputs.",
              "instructor_notes": ""
            },
            {
              "id": 198049,
              "key": "1304864a-e067-4251-8ba2-375d46092b7e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now that I've given you the equation for the sigmoid function, I want you to add it to the `MiniFlow` library. To do so, you'll want to use `np.exp` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)) to make your life much easier.\n\nYou'll be using `Sigmoid` in conjunction with `Linear`. Here's how it should look:",
              "instructor_notes": ""
            },
            {
              "id": 198053,
              "key": "3bd02e82-bfc4-4381-a5c7-59840245ed5c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/58114a6a_screen-shot-2016-10-26-at-19.28.34/screen-shot-2016-10-26-at-19.28.34.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3bd02e82-bfc4-4381-a5c7-59840245ed5c",
              "caption": "Inputs > Linear Transform > Sigmoid",
              "alt": null,
              "width": 1846,
              "height": 348,
              "instructor_notes": null
            },
            {
              "id": 198052,
              "key": "97fd698c-41e4-4814-a5a3-d3dd5fb1d78e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Instructions\n\n1. Open nn.py to see how the network will use `Sigmoid`.\n2. Open miniflow.py. Modify the `forward` method of the `Sigmoid` class to reflect the sigmoid function's behavior.\n3. Test your work! Hit \"Submit\" when your `Sigmoid` works as expected.",
              "instructor_notes": ""
            },
            {
              "id": 197340,
              "key": "bac048da-f6a9-4e4b-8bad-e046fd92db53",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6698921718054912",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nThis network feeds the output of a linear transform\nto the sigmoid function.\n\nFinish implementing the Sigmoid class in miniflow.py!\n\nFeel free to play around with this network, too!\n\"\"\"\n\nimport numpy as np\nfrom miniflow import *\n\nX, W, b = Input(), Input(), Input()\n\nf = Linear(X, W, b)\ng = Sigmoid(f)\n\nX_ = np.array([[-1., -2.], [-1, -2]])\nW_ = np.array([[2., -3], [2., -3]])\nb_ = np.array([-3., -5])\n\nfeed_dict = {X: X_, W: W_, b: b_}\n\ngraph = topological_sort(feed_dict)\noutput = forward_pass(g, graph)\n\n\"\"\"\nOutput should be:\n[[  1.23394576e-04   9.82013790e-01]\n [  1.23394576e-04   9.82013790e-01]]\n\"\"\"\nprint(output)\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nFix the Sigmoid class so that it computes the sigmoid function\non the forward pass!\n\nScroll down to get started.\n\"\"\"\n\nimport numpy as np\n\nclass Node(object):\n    def __init__(self, inbound_nodes=[]):\n        self.inbound_nodes = inbound_nodes\n        self.value = None\n        self.outbound_nodes = []\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward():\n        raise NotImplementedError\n\n\nclass Input(Node):\n    def __init__(self):\n        # An Input node has no inbound nodes,\n        # so no need to pass anything to the Node instantiator\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n\nclass Linear(Node):\n    def __init__(self, X, W, b):\n        # Notice the ordering of the input nodes passed to the\n        # Node constructor.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n\n\nclass Sigmoid(Node):\n    \"\"\"\n    You need to fix the `_sigmoid` and `forward` methods.\n    \"\"\"\n    def __init__(self, node):\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used later with `backward` as well.\n\n        `x`: A numpy array-like object.\n\n        Return the result of the sigmoid function.\n\n        Your code here!\n        \"\"\"\n\n\n    def forward(self):\n        \"\"\"\n        Set the value of this node to the result of the\n        sigmoid function, `_sigmoid`.\n\n        Your code here!\n        \"\"\"\n        # This is a dummy value to prevent numpy errors\n        # if you test without changing this method.\n        self.value = -1\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(output_node, sorted_nodes):\n    \"\"\"\n    Performs a forward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n        `sorted_nodes`: a topologically sorted list of nodes.\n\n    Returns the output node's value\n    \"\"\"\n\n    for n in sorted_nodes:\n        n.forward()\n\n    return output_node.value\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 197664,
          "key": "60049bb7-bdf9-44bc-a6bc-6c8679b62de7",
          "title": "Cost",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 197666,
              "key": "58a98137-ba68-4981-a62d-25e8d64976a7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's how I implemented the sigmoid function.\n\n```\nclass Sigmoid(Node):\n    def __init__(self, node):\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n\n    def forward(self):\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n```\n\nIt may have seemed strange that `_sigmoid` was a separate method. As seen in the derivative of the sigmoid function, Equation (4), the sigmoid function is actually *a part of its own derivative*. Keeping `_sigmoid` separate means you won't have to implement it twice for forward and backward propagations.\n\nThis is exciting! At this point, you have used weights and biases to compute outputs. And you've used an activation function to categorize the output. As you may recall, neural networks improve the **accuracy** of their outputs by modifying weights and biases in response to training against labeled datasets.",
              "instructor_notes": ""
            },
            {
              "id": 197667,
              "key": "6b443c01-d30b-4da4-9ad0-ce8c2aa559ad",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "There are many techniques for defining the accuracy of a neural network, all of which center on the network's ability to produce values that come as close as possible to known correct values. People use different names for this accuracy measurement, often terming it **loss** or **cost**. I'll use the term *cost* most often.\n\nFor this lab, you will calculate the cost using the mean squared error (MSE). It looks like so:",
              "instructor_notes": ""
            },
            {
              "id": 288361,
              "key": "11b4b539-ece4-4676-93b9-d18061b8ff87",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "<div id=\"container\" style=\"text-align: center\">\n    <span class=\"mathquill\">\\LARGE C(w, b) = \\frac{1}{m}\\sum_x || y(x) -  a   || ^2</span>\n\nEquation (5)\n<div id=\"container\">",
              "instructor_notes": ""
            },
            {
              "id": 197672,
              "key": "e1e9c3b6-48a9-473d-a992-df17a1d89704",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here <span class='mathquill'>w</span> denotes the collection of all weights in the network, <span class='mathquill'>b</span> all the biases, <span class='mathquill'>m</span> is the total number of training examples, and <span class='mathquill'>a</span> is the approximation of <span class='mathquill'>y(x)</span> by the network. Note that both <span class='mathquill'>a</span> and <span class='mathquill'>y(x)</span> are vectors of the same length.\n\nThe collection of weights is all the weight matrices flattened into vectors and concatenated to one big vector. The same goes for the collection of biases except they're already vectors so there's no need to flatten them prior to the concatenation.\n\nHere's an example of creating <span class='mathquill'>w</span> in code:\n\n```python\n# 2 by 2 matrices\nw1  = np.array([[1, 2], [3, 4]])\nw2  = np.array([[5, 6], [7, 8]])\n\n# flatten\nw1_flat = np.reshape(w1, -1)\nw2_flat = np.reshape(w2, -1)\n\nw = np.concatenate((w1_flat, w2_flat))\n# array([1, 2, 3, 4, 5, 6, 7, 8])\n```\n\nIt's a nice way to abstract all the weights and biases used in the neural network and makes some things easier to write as we'll see soon in the upcoming gradient descent sections.\n\n**NOTE:** It's not required you do this in your code! It's just easier to do this talk about the weights and biases as a collective than consider them invidually.\n\nThe cost, <span class='mathquill'>C</span>, depends on the difference between the correct output, <span class='mathquill'>y(x)</span>, and the network's output, <span class='mathquill'>a</span>. It's easy to see that no difference between <span class='mathquill'>y(x)</span> and <span class='mathquill'>a</span> (for all values of <span class='mathquill'>x</span>) leads to a cost of **0**.\n\nThis is the ideal situation, and in fact the learning process revolves around minimizing the cost as much as possible.\n\nI want you to calculate the cost now.\n\nYou implemented this network in the forward direction in the last quiz.\n\nAs it stands right now, it outputs gibberish. The activation of the sigmoid node means nothing because the network has no labeled output against which to compare. Furthermore, the weights and bias cannot change and learning cannot happen without a cost.\n\n### Instructions\n\nFor this quiz, you will run the forward pass against the network in nn.py. I want you to finish implementing the `MSE` method so that it calculates the cost from the equation above.\n\nI recommend using the `np.square` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.square.html)) method to make your life easier.\n\n1. Check out nn.py to see how `MSE` will calculate the cost.\n2. Open miniflow.py. Finish building `MSE`.\n3. Test your network! See if the cost makes sense given the inputs by playing with nn.py.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 197752,
              "key": "aae5ed12-0d23-4b4b-9b2c-492ca696c5ad",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "MSE",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5136424829583360",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nTest your MSE method with this script!\n\nNo changes necessary, but feel free to play\nwith this script to test your network.\n\"\"\"\n\nimport numpy as np\nfrom miniflow import *\n\ny, a = Input(), Input()\ncost = MSE(y, a)\n\ny_ = np.array([1, 2, 3])\na_ = np.array([4.5, 5, 10])\n\nfeed_dict = {y: y_, a: a_}\ngraph = topological_sort(feed_dict)\n# forward pass\nforward_pass(graph)\n\n\"\"\"\nExpected output\n\n23.4166666667\n\"\"\"\nprint(cost.value)\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "import numpy as np\n\n\nclass Node(object):\n    \"\"\"\n    Base class for nodes in the network.\n\n    Arguments:\n\n        `inbound_nodes`: A list of nodes with edges into this node.\n    \"\"\"\n    def __init__(self, inbound_nodes=[]):\n        \"\"\"\n        Node's constructor (runs when the object is instantiated). Sets\n        properties that all nodes need.\n        \"\"\"\n        # A list of nodes with edges into this node.\n        self.inbound_nodes = inbound_nodes\n        # The eventual value of this node. Set by running\n        # the forward() method.\n        self.value = None\n        # A list of nodes that this node outputs to.\n        self.outbound_nodes = []\n        # Sets this node as an outbound node for all of\n        # this node's inputs.\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `forward` method.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Input(Node):\n    \"\"\"\n    A generic input into the network.\n    \"\"\"\n    def __init__(self):\n        # The base class constructor has to run to set all\n        # the properties here.\n        #\n        # The most important property on an Input is value.\n        # self.value is set during `topological_sort` later.\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n\nclass Linear(Node):\n    \"\"\"\n    Represents a node that performs a linear transform.\n    \"\"\"\n    def __init__(self, X, W, b):\n        # The base class (Node) constructor. Weights and bias\n        # are treated like inbound nodes.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        \"\"\"\n        Performs the math behind a linear transform.\n        \"\"\"\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n\n\nclass Sigmoid(Node):\n    \"\"\"\n    Represents a node that performs the sigmoid activation function.\n    \"\"\"\n    def __init__(self, node):\n        # The base class constructor.\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x))\n\n    def forward(self):\n        \"\"\"\n        Perform the sigmoid function and set the value.\n        \"\"\"\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n\n\nclass MSE(Node):\n    def __init__(self, y, a):\n        \"\"\"\n        The mean squared error cost function.\n        Should be used as the last node for a network.\n        \"\"\"\n        # Call the base class' constructor.\n        Node.__init__(self, [y, a])\n\n    def forward(self):\n        \"\"\"\n        Calculates the mean squared error.\n        \"\"\"\n        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n        # errors.\n        #\n        # For example, if we subtract an array of shape (3,) from an array of shape\n        # (3,1) we get an array of shape(3,3) as the result when we want\n        # an array of shape (3,1) instead.\n        #\n        # Making both arrays (3,1) insures the result is (3,1) and does\n        # an elementwise subtraction as expected.\n        y = self.inbound_nodes[0].value.reshape(-1, 1)\n        a = self.inbound_nodes[1].value.reshape(-1, 1)\n        # TODO: your code here\n        pass\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_pass(graph):\n    \"\"\"\n    Performs a forward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `graph`: The result of calling `topological_sort`.\n    \"\"\"\n    # Forward pass\n    for n in graph:\n        n.forward()\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 264027,
          "key": "a6e33c40-57b4-4f29-9bef-57d5f7e93643",
          "title": "Cost Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 264028,
              "key": "e410d4a0-6833-44d0-872b-c19adeca9448",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's how I implemented `MSE`:\n\n```python\nclass MSE(Node):\n    def __init__(self, y, a):\n        \"\"\"\n        The mean squared error cost function.\n        Should be used as the last node for a network.\n        \"\"\"\n        # Call the base class' constructor.\n        Node.__init__(self, [y, a])\n\n    def forward(self):\n        \"\"\"\n        Calculates the mean squared error.\n        \"\"\"\n        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n        # errors.\n        #\n        # For example, if we subtract an array of shape (3,) from an array of shape\n        # (3,1) we get an array of shape(3,3) as the result when we want\n        # an array of shape (3,1) instead.\n        #\n        # Making both arrays (3,1) insures the result is (3,1) and does\n        # an elementwise subtraction as expected.\n        y = self.inbound_nodes[0].value.reshape(-1, 1)\n        a = self.inbound_nodes[1].value.reshape(-1, 1)\n        m = self.inbound_nodes[0].value.shape[0]\n\n        diff = y - a\n        self.value = np.mean(diff**2)\n```\n\nThe math behind `MSE` reflects Equation (5), where *y* is target output and *a* is output computed by the neural network. We then square the difference `diff**2`, alternatively, this could be `np.square(diff)`. Lastly we need to sum the squared differences and divide by the total number of examples *m*. This can be achieved in with `np.mean` or `(1 /m) * np.sum(diff**2)`.\n\nNote the order of `y` and `a` doesn't actually matter, we could switch them around (`a - y`) and get the same value.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 264030,
          "key": "ce4e18b2-2777-40ca-9514-c659ada3f09a",
          "title": "Gradient Descent",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 264031,
              "key": "cbffd951-ffb0-42e7-a0f8-f1e8465f2e28",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Journey to the Bottom of the Valley\n\nHere I'll give you a little refresher on gradient descent so we can start training our network with MiniFlow. Remember that our goal is to make our network output as close as possible to the target values by minimizing the cost. You can envision the cost as a hill or mountain and we want to get to the bottom.\n\nImagine your model parameters are represented by a ball sitting on a hill. Intuitively, we want to push the ball downhill. And that makes sense, but when we're talking about our cost function, how do we know which way is downhill? \n\nLuckily, the gradient provides this exact information. \n\nTechnically, the gradient actually points uphill, in the direction of **steepest ascent**. But if we put a `-` sign in front of this value, we get the direction of **steepest descent**, which is what we want.\n\nYou'll learn more about the gradient in a moment, but, for now, just think of it as a vector of numbers. Each number represents the amount by which we should adjust a corresponding weight or bias in the neural network. Adjusting all of the weights and biases by the gradient values reduces the cost (or error) of the network.\n\nGot all that?\n\nGreat! Now we know where to push the ball. The next thing to consider is how much force should be applied to the _push_. This is known as the _learning rate_, which is an apt name since this value determines how quickly or slowly the neural network learns. \n\nYou might be tempted to set a really big learning rate, so the network learns really fast, right? \n\nBe careful! If the value is too large you could overshoot the target and eventually diverge. Yikes!",
              "instructor_notes": ""
            },
            {
              "id": 264032,
              "key": "c75210b5-3250-4884-9161-854265644b01",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/December/5840d0aa_gradient-descent-convergence/gradient-descent-convergence.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c75210b5-3250-4884-9161-854265644b01",
              "caption": "**Convergence**. This is the ideal behaviour.",
              "alt": null,
              "width": 320,
              "height": 180,
              "instructor_notes": null
            },
            {
              "id": 264033,
              "key": "3eae2c9b-6c0a-4bfb-8d2f-d17c44676792",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/December/5840d077_gradient-descent-divergence/gradient-descent-divergence.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3eae2c9b-6c0a-4bfb-8d2f-d17c44676792",
              "caption": "**Divergence**. This can happen when the learning rate is too large.",
              "alt": null,
              "width": 320,
              "height": 180,
              "instructor_notes": null
            },
            {
              "id": 264034,
              "key": "2b8dcb69-cc37-4945-892a-b7f1b705e63e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So what is a good learning rate, then? \n\nThis is more of a guessing game than anything else but empirically values in the range 0.1 to 0.0001 work well. The range 0.001 to 0.0001 is popular, as 0.1 and 0.01 are sometimes too large.\n\nHere's the formula for gradient descent (pseudocode):\n\n```\nx = x - learning_rate * gradient_of_x\n```\n\n`x` is a parameter used by the neural network (i.e. a single weight or bias).\n\nWe multiply `gradient_of_x` (the uphill direction) by `learning_rate` (the force of the push) and then subtract that from `x` to make the push go downhill.\n\nAwesome! Time to apply all this in a quiz.",
              "instructor_notes": ""
            },
            {
              "id": 264035,
              "key": "f1e8dd0d-07ec-414a-853b-a3094d93e533",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Setup\n\nFor this quiz you'll complete `TODOs` in both the `f.py` and `gd.py` files.\n\nTasks:\n\n* Set the `learning_rate` in `f.py`.\n* Complete the gradient descent implementation in `gradient_descent_update` function in `gd.py`.\n\nNotes:\n\n* Setting the `learning_rate` to 0.1 should result in `x` -> 0 and `f(x)` -> 5 if you've implemented gradient descent correctly.\n* Play around with different values for the learning rate. Try very small values, values close to 1, above 1, etc. What happens?",
              "instructor_notes": ""
            },
            {
              "id": 264036,
              "key": "e9d11b31-19cc-4559-a43c-b7e8f614a77a",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5279017746956288",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nGiven the starting point of any `x` gradient descent\nshould be able to find the minimum value of x for the\ncost function `f` defined below.\n\"\"\"\nimport random\nfrom gd import gradient_descent_update\n\n\ndef f(x):\n    \"\"\"\n    Quadratic function.\n\n    It's easy to see the minimum value of the function\n    is 5 when is x=0.\n    \"\"\"\n    return x**2 + 5\n\n\ndef df(x):\n    \"\"\"\n    Derivative of `f` with respect to `x`.\n    \"\"\"\n    return 2*x\n\n\n# Random number between 0 and 10,000. Feel free to set x whatever you like.\nx = random.randint(0, 10000)\n# TODO: Set the learning rate\nlearning_rate = None\nepochs = 100\n\nfor i in range(epochs+1):\n    cost = f(x)\n    gradx = df(x)\n    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n    x = gradient_descent_update(x, gradx, learning_rate)\n",
                    "name": "f.py"
                  },
                  {
                    "text": "def gradient_descent_update(x, gradx, learning_rate):\n    \"\"\"\n    Performs a gradient descent update.\n    \"\"\"\n    # TODO: Implement gradient descent.\n    \n    # Return the new value for x\n    return x\nimport f",
                    "name": "gd.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 264023,
          "key": "aaeef9a2-f825-4982-ba9d-2aeb8fd615c9",
          "title": "Backpropagation",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 264024,
              "key": "b935479f-0b27-4cbd-ac8c-19cbb4baa15d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Gradient Descent Solution\n\n```python\ndef gradient_descent_update(x, gradx, learning_rate):\n    \"\"\"\n    Performs a gradient descent update.\n    \"\"\"\n    x = x - learning_rate * gradx\n    # Return the new value for x\n    return x\n```\n\nWe adjust the old `x` pushing it in the _opposite direction_ of `gradx` with the _force_ `learning_rate`. Subtracting `learning_rate * gradx`. Remember the gradient is initially in the direction of **steepest ascent** so subtracting `learning_rate * gradx` from `x` turns it into **steepest descent**. You can make sure of this yourself by replacing the subtraction with an addition.",
              "instructor_notes": ""
            },
            {
              "id": 264025,
              "key": "91e383b8-cade-4677-b73e-ab20a17c06b9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### The Gradient & Backpropagation\n\nNow that we know how to update our weights and biases using the gradient, we need to figure out how to calculate the gradients for all of our nodes. For each node, we'll want to change the values based on the gradient of the cost with respect to the value of that node. In this way, the gradient descent updates we make will eventually converge to the minimum of the cost.\n\nLet's consider a network with a linear node <span class='mathquill'>l_1</span>, a sigmoid node <span class='mathquill'>s</span>, and another linear node <span class='mathquill'>l_2</span>, followed by an MSE node to calculate the cost, <span class='mathquill'>C</span>.",
              "instructor_notes": ""
            },
            {
              "id": 264040,
              "key": "4c0de1f9-f497-4489-91fa-5bd7e6c71339",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cc251_two-layer-graph/two-layer-graph.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4c0de1f9-f497-4489-91fa-5bd7e6c71339",
              "caption": "Forward pass for a simple two layer network.",
              "alt": null,
              "width": 1188,
              "height": 366,
              "instructor_notes": null
            },
            {
              "id": 264038,
              "key": "7fa1121b-7fd9-4c00-912b-ad52be836a66",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Writing this out in MiniFlow, it would look like:\n\n```python\nX, y = Input(), Input()\nW1, b1 = Input(), Input()\nW2, b2 = Input(), Input()\n\nl1 = Linear(X, W1, b1)\ns = Sigmoid(l1)\nl2 = Linear(s, W2, b2)\ncost = MSE(l2, y)\n```",
              "instructor_notes": ""
            },
            {
              "id": 264041,
              "key": "d9875a77-d19c-427c-888c-b149cea47524",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We can see that each of the values of these nodes flows forwards and eventually produces the cost <span class='mathquill'>C</span>. For example, the value of the second linear node <span class='mathquill'>l_2</span> goes into the cost node and determines the value of that node. Accordingly, a *change* in <span class='mathquill'>l_2</span> will produce a *change* in <span class='mathquill'>C</span>. We can write this relationship between the changes as a gradient,",
              "instructor_notes": ""
            },
            {
              "id": 264043,
              "key": "4b931419-f94e-4fb0-84dc-9510f3ecf007",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cc3f0_dcdl2/dcdl2.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4b931419-f94e-4fb0-84dc-9510f3ecf007",
              "caption": "",
              "alt": null,
              "width": 47,
              "height": 78,
              "instructor_notes": null
            },
            {
              "id": 264044,
              "key": "05501d53-3094-46ba-bc8c-b3785c8b014c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "This is what a gradient means, it's a slope, how much you change the cost <span class='mathquill'>\\partial C</span> given a change in <span class='mathquill'>l_2</span>,  <span class='mathquill'>\\partial l_2</span>. So a node with a larger gradient with respect to the cost is going to contribute a larger change to the cost. In this way, we can assign blame for the cost to each node. The larger the gradient for a node, the more blame it gets for the final cost. And the more blame a node has, the more we'll update it in the gradient descent step.\n\nIf we want to update one of the weights with gradient descent, we'll need the gradient of the cost with respect to those weights. Let's see how we can use this framework to find the gradient for the weights in the second layer, <span class='mathquill'>w_2</span>. We want to calculate the gradient of <span class='mathquill'>C</span> with respect to <span class='mathquill'>w_2</span>:\n\n",
              "instructor_notes": ""
            },
            {
              "id": 264045,
              "key": "e304f285-a447-4765-82e9-d3160c040ffa",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cc936_dcdw2/dcdw2.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e304f285-a447-4765-82e9-d3160c040ffa",
              "caption": "",
              "alt": null,
              "width": 84,
              "height": 111,
              "instructor_notes": null
            },
            {
              "id": 264046,
              "key": "51c311b5-b02f-400a-a502-ff602dc357bf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We can see in the graph that <span class=\"mathquill\">w_2</span> is connected to <span class=\"mathquill\">l_2</span>, so a change in <span class=\"mathquill\">w_2</span> is going to create a change in <span class=\"mathquill\">l_2</span> which then creates a change in <span class=\"mathquill\">C</span>. We can assign blame to <span class=\"mathquill\">w_2</span> by sending the cost gradient back through the network. First you have how much <span class=\"mathquill\">l_2</span> affected <span class=\"mathquill\">C</span>, then how much <span class=\"mathquill\">w_2</span> affected <span class=\"mathquill\">l_2</span>. Multiplying these gradients together gets you the total blame attributed to <span class=\"mathquill\">w_2</span>.",
              "instructor_notes": ""
            },
            {
              "id": 264056,
              "key": "ab6f39dc-54db-4fc3-852b-b5ced001d6dc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cd6b7_w2-backprop-graph/w2-backprop-graph.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ab6f39dc-54db-4fc3-852b-b5ced001d6dc",
              "caption": "",
              "alt": null,
              "width": 1130,
              "height": 386,
              "instructor_notes": null
            },
            {
              "id": 264121,
              "key": "b81159b2-bca7-4af4-809c-9f1357628140",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "#### Pre-requisites\n\nBelow we're getting into the math behind backpropagation which requires multivariable calculus. If you need a refresher, I highly recommend checking out\n\n*  [Khan Academy's lessons on partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction) \n* Another video on [gradients](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) \n* And finally, using [the chain rule](https://www.khanacademy.org/math/ap-calculus-ab/product-quotient-chain-rules-ab/chain-rule-ab/v/chain-rule-introduction)\n\n#### Continuing on\nMultiplying these gradients is just an application of the chain rule:",
              "instructor_notes": ""
            },
            {
              "id": 264047,
              "key": "9bc22073-8ca3-448e-bf58-a5474758ccb7",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cc99a_dcdw2-chain/dcdw2-chain.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9bc22073-8ca3-448e-bf58-a5474758ccb7",
              "caption": "",
              "alt": null,
              "width": 224,
              "height": 78,
              "instructor_notes": null
            },
            {
              "id": 264111,
              "key": "d19f9c20-f3fa-4ad0-8257-11335bd3b98d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You can see in the graph <span class=\"mathquill\">w_2</span>, <span class=\"mathquill\">l_2</span>, and <span class=\"mathquill\">C</span> are chained together. Any change in <span class=\"mathquill\">w_2</span> will create a change in <span class=\"mathquill\">l_2</span> and the size of that change is given by the gradient <span class=\"mathquill\">\\partial l_2 / \\partial w_2</span>. Now, since <span class=\"mathquill\">l_2</span> is changing this will cause a change in the cost <span class=\"mathquill\">C</span> and the size of that change is given by the gradient <span class=\"mathquill\">\\partial C / \\partial l_2</span>. You can think of the chain rule similarly to the domino effect, changing something in the network will propagate through it altering other nodes along the way.",
              "instructor_notes": ""
            },
            {
              "id": 264048,
              "key": "f1a11e36-68f5-4aee-82dd-dc369116a4b3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you think of the chain rule as normal fractions, you can see that <span class=\"mathquill\"> \\partial l_2</span> in the denominator and numerator cancel out and you get back <span class=\"mathquill\"> \\partial C / \\partial w_2</span> (although it doesn't exactly work like normal fractions, but it helps to keep track of things.) Okay, let's work out the gradient for <span class=\"mathquill\">w_2</span>. First, we need to know the gradient for <span class=\"mathquill\"> l_2</span>.\n\nAs a reminder [the cost](https://classroom.udacity.com/nanodegrees/nd101/parts/2a9dba0b-28eb-4b0e-acfa-bdcf35680d90/modules/269a4aad-9025-4354-a0ea-2623e889540a/lessons/b6deebe4-7f78-4947-b2c6-fc660ca942fb/concepts/60049bb7-bdf9-44bc-a6bc-6c8679b62de7) is\n\n",
              "instructor_notes": ""
            },
            {
              "id": 264126,
              "key": "e2fd8726-5bbf-41dd-83b0-f2816ec9fef5",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589d1706_cost/cost.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e2fd8726-5bbf-41dd-83b0-f2816ec9fef5",
              "caption": "",
              "alt": null,
              "width": 332,
              "height": 87,
              "instructor_notes": null
            },
            {
              "id": 264123,
              "key": "010b805e-ec08-4567-b7a1-012b8eba07b4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "And the value for the second linear node is\n\n",
              "instructor_notes": ""
            },
            {
              "id": 264125,
              "key": "94adb5c7-95eb-4068-88c0-29bb4ecc5b82",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589d15a7_l2/l2.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/94adb5c7-95eb-4068-88c0-29bb4ecc5b82",
              "caption": "",
              "alt": null,
              "width": 216,
              "height": 29,
              "instructor_notes": null
            },
            {
              "id": 264124,
              "key": "fd31aca8-dd07-4619-b5ab-98e6afb3ab14",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "where <span class='mathquill'>w_2</span>, <span class='mathquill'> s </span>, and <span class='mathquill'> b_2</span> are all vectors and <span class='mathquill'>w_2 \\cdot s </span> means the dot product of <span class='mathquill'>w_2</span> and <span class='mathquill'> s </span>.",
              "instructor_notes": ""
            },
            {
              "id": 264589,
              "key": "3f73695e-47ce-4c10-9712-9ef5f909bfa0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/58a24b96_dcdl2-grad-fixed/dcdl2-grad-fixed.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3f73695e-47ce-4c10-9712-9ef5f909bfa0",
              "caption": "",
              "alt": null,
              "width": 448,
              "height": 202,
              "instructor_notes": null
            },
            {
              "id": 264120,
              "key": "25e0acb3-a56c-41ab-9d41-f61ff8685776",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589d0ee1_dl2dw2-grad/dl2dw2-grad.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/25e0acb3-a56c-41ab-9d41-f61ff8685776",
              "caption": "",
              "alt": null,
              "width": 343,
              "height": 119,
              "instructor_notes": null
            },
            {
              "id": 264051,
              "key": "3f602bf8-9b98-4b18-93f2-60caad6165a1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "And putting these together, you get the gradient for <span class='mathquill'>w_2</span>",
              "instructor_notes": ""
            },
            {
              "id": 266942,
              "key": "56f6c955-5eb4-43df-8d79-7c167cdb20ea",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/58a51cfe_dcdw2-grad-fixed/dcdw2-grad-fixed.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/56f6c955-5eb4-43df-8d79-7c167cdb20ea",
              "caption": "",
              "alt": null,
              "width": 372,
              "height": 86,
              "instructor_notes": null
            },
            {
              "id": 264055,
              "key": "1e61af97-d098-403a-8554-0168dcbcfe88",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "This is the gradient you use in the gradient descent update for <span class='mathquill'>w_2</span>. You can see what we did here, we walked back through the graph and multiplied all the gradients we found along the way.",
              "instructor_notes": ""
            },
            {
              "id": 264053,
              "key": "8ef132e1-834b-496d-abf4-ee046f537406",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now, let's go deeper and calculate the gradient for <span class='mathquill'>w_1</span>. Here we use the same method as before, walking backwards through the graph.",
              "instructor_notes": ""
            },
            {
              "id": 264064,
              "key": "73306b62-f273-410d-a072-055a75a24706",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cda0d_w1-backprop-graph/w1-backprop-graph.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/73306b62-f273-410d-a072-055a75a24706",
              "caption": "",
              "alt": null,
              "width": 1130,
              "height": 386,
              "instructor_notes": null
            },
            {
              "id": 264070,
              "key": "319b06c9-f017-440e-8f3c-a320842d3476",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Hopefully it's clear now how to write out the gradient for  <span class='mathquill'>w_1</span> just by looking at the graph. Using the chain rule, we'll write out the gradients for each node going backwards through the graph until we get to <span class='mathquill'>w_1</span>. ",
              "instructor_notes": ""
            },
            {
              "id": 264066,
              "key": "0750334c-71f8-43d6-a090-934cc38b3042",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cda1e_dcdw1-chain/dcdw1-chain.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0750334c-71f8-43d6-a090-934cc38b3042",
              "caption": "",
              "alt": null,
              "width": 325,
              "height": 78,
              "instructor_notes": null
            },
            {
              "id": 264107,
              "key": "79175883-1a53-479f-a807-8d341bb8fb25",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now we can start calculating each gradient in this expression to get the gradient for <span class='mathquill'>w_1</span>",
              "instructor_notes": ""
            },
            {
              "id": 264117,
              "key": "62d2edec-eaa7-40e7-b927-40c47a19d13f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cfa74_dl2ds-grad/dl2ds-grad.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/62d2edec-eaa7-40e7-b927-40c47a19d13f",
              "caption": "",
              "alt": null,
              "width": 307,
              "height": 119,
              "instructor_notes": null
            },
            {
              "id": 264089,
              "key": "55ef83db-f815-4980-a0f9-ecbeea6267a6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The next part is the gradient of the sigmoid function, <span class='mathquill'>s = f(l_1)</span>. Since we're using the logistic function here, the derivative can be written in terms of the sigmoid itself\n",
              "instructor_notes": ""
            },
            {
              "id": 264080,
              "key": "fac4ae78-2e89-4c79-a8e8-5789a47b0e5b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cde9e_dsdl1/dsdl1.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fac4ae78-2e89-4c79-a8e8-5789a47b0e5b",
              "caption": "",
              "alt": null,
              "width": 319,
              "height": 178,
              "instructor_notes": null
            },
            {
              "id": 264109,
              "key": "13a4c66a-a743-499b-930f-bbe4f6b97531",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/589cef93_dl1dw1-grad/dl1dw1-grad.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/13a4c66a-a743-499b-930f-bbe4f6b97531",
              "caption": "",
              "alt": null,
              "width": 346,
              "height": 119,
              "instructor_notes": null
            },
            {
              "id": 264087,
              "key": "88ded8b4-e53e-4d89-9ac4-a7a3005bc357",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Putting this all together, you get",
              "instructor_notes": ""
            },
            {
              "id": 264590,
              "key": "c90f8570-7d38-49f9-9a11-95fb3912cef8",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/February/58a24c7c_dcdw1-grad-fixed/dcdw1-grad-fixed.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c90f8570-7d38-49f9-9a11-95fb3912cef8",
              "caption": "",
              "alt": null,
              "width": 750,
              "height": 102,
              "instructor_notes": null
            },
            {
              "id": 264072,
              "key": "7d3a09f8-1a8f-4022-95f8-95259ad08d6f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now we can see a clear pattern. To find the gradient, you just multiply the gradients for all nodes in front of it going backwards from the cost. This is the idea behind **backpropagation**. The gradients are passed backwards through the network and used with gradient descent to update the weights and biases. If a node has multiple outgoing nodes, you just sum up the gradients from each node.\n\n### Implementing in MiniFlow\n\nLet's think about how to translate this into MiniFlow. Looking at the graph, you see that each node gets the cost gradient from it's outbound nodes. For example, the node <span class=\"mathquill\">l_1</span> gets <span class=\"mathquill\">\\partial C / \\partial l_1</span> through the sigmoid node, <span class=\"mathquill\">s</span>. Then <span class=\"mathquill\">l_1</span> passes on the cost gradient to the weight node <span class=\"mathquill\">w_1</span>, but multiplied by <span class=\"mathquill\">\\partial l_1 / \\partial w_1</span>, the gradient of <span class=\"mathquill\">l_1</span> with respect to it's input <span class=\"mathquill\">w_1</span>.\n\nSo, each node will pass on the cost gradient to its inbound nodes and each node will get the cost gradient from it's outbound nodes. Then, for each node we'll need to calculate a gradient that's the cost gradient times the gradient of that node with respect to its inputs. Below I've written out this process for a `Linear` node. \n\n```python\n# Initialize a partial for each of the inbound_nodes.\nself.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n# Cycle through the outputs. The gradient will change depending\n# on each output, so the gradients are summed over all outputs.\nfor n in self.outbound_nodes:\n    # Get the partial of the cost with respect to this node.\n    grad_cost = n.gradients[self]\n    # Set the partial of the loss with respect to this node's inputs.\n    self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n    # Set the partial of the loss with respect to this node's weights.\n    self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n    # Set the partial of the loss with respect to this node's bias.\n    self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n```",
              "instructor_notes": ""
            },
            {
              "id": 264039,
              "key": "4e84dde3-cf88-4212-8fc4-930a4a26d040",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### New Code\n\nThere have been a couple of changes to MiniFlow since we last took it for a spin:\n\nThe first being the `Node` class now has a `backward` method, as well as a new attribute `self.gradients`, which is used to store and cache gradients during the backward pass.\n\n```python\nclass Node(object):\n    \"\"\"\n    Base class for nodes in the network.\n\n    Arguments:\n\n        `inbound_nodes`: A list of nodes with edges into this node.\n    \"\"\"\n    def __init__(self, inbound_nodes=[]):\n        \"\"\"\n        Node's constructor (runs when the object is instantiated). Sets\n        properties that all nodes need.\n        \"\"\"\n        # A list of nodes with edges into this node.\n        self.inbound_nodes = inbound_nodes\n        # The eventual value of this node. Set by running\n        # the forward() method.\n        self.value = None\n        # A list of nodes that this node outputs to.\n        self.outbound_nodes = []\n        # New property! Keys are the inputs to this node and\n        # their values are the partials of this node with\n        # respect to that input.\n        self.gradients = {}\n        # Sets this node as an outbound node for all of\n        # this node's inputs.\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `forward` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `backward` method.\n        \"\"\"\n        raise NotImplementedError\n\n```\n\nThe second change is to the helper function `forward_pass()`. That function has been replaced with `forward_and_backward()`.\n\n\n```python\ndef forward_and_backward(graph):\n    \"\"\"\n    Performs a forward pass and a backward pass through a list of sorted nodes.\n\n    Arguments:\n\n        `graph`: The result of calling `topological_sort`.\n    \"\"\"\n    # Forward pass\n    for n in graph:\n        n.forward()\n\n    # Backward pass\n    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n    for n in graph[::-1]:\n        n.backward()\n```",
              "instructor_notes": ""
            },
            {
              "id": 264114,
              "key": "daf4655b-e387-4e3e-8ee1-8ecb8dc1a381",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Setup\n\nHere's the derivative of the <span class=\"mathquill\">sigmoid</span> function w.r.t <span class=\"mathquill\">x</span>:\n\n<span class=\"mathquill\">sigmoid(x) = 1 / (1 + exp(-x))</span>\n\n<span class=\"mathquill\">\n\\frac {\\partial sigmoid}{\\partial x} = sigmoid(x) * (1 - sigmoid(x))\n</span>\n\n\n* Complete the implementation of backpropagation for the `Sigmoid` node by finishing the `backward` method in `miniflow.py`.\n* The `backward` methods for all other nodes have already been implemented. Taking a look at them might be helpful.",
              "instructor_notes": ""
            },
            {
              "id": 264116,
              "key": "a657bdbb-2dd0-4582-ac92-48792fed5cfc",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6069650560516096",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nTest your network here!\n\nNo need to change this code, but feel free to tweak it\nto test your network!\n\nMake your changes to backward method of the Sigmoid class in miniflow.py\n\"\"\"\n\nimport numpy as np\nfrom miniflow import *\n\nX, W, b = Input(), Input(), Input()\ny = Input()\nf = Linear(X, W, b)\na = Sigmoid(f)\ncost = MSE(y, a)\n\nX_ = np.array([[-1., -2.], [-1, -2]])\nW_ = np.array([[2.], [3.]])\nb_ = np.array([-3.])\ny_ = np.array([1, 2])\n\nfeed_dict = {\n    X: X_,\n    y: y_,\n    W: W_,\n    b: b_,\n}\n\ngraph = topological_sort(feed_dict)\nforward_and_backward(graph)\n# return the gradients for each Input\ngradients = [t.gradients[t] for t in [X, y, W, b]]\n\n\"\"\"\nExpected output\n\n[array([[ -3.34017280e-05,  -5.01025919e-05],\n       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n       [ 1.9999833]]), array([[  5.01028709e-05],\n       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n\"\"\"\nprint(gradients)\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "\"\"\"\nImplement the backward method of the Sigmoid node.\n\"\"\"\nimport numpy as np\n\n\nclass Node(object):\n    \"\"\"\n    Base class for nodes in the network.\n\n    Arguments:\n\n        `inbound_nodes`: A list of nodes with edges into this node.\n    \"\"\"\n    def __init__(self, inbound_nodes=[]):\n        \"\"\"\n        Node's constructor (runs when the object is instantiated). Sets\n        properties that all nodes need.\n        \"\"\"\n        # A list of nodes with edges into this node.\n        self.inbound_nodes = inbound_nodes\n        # The eventual value of this node. Set by running\n        # the forward() method.\n        self.value = None\n        # A list of nodes that this node outputs to.\n        self.outbound_nodes = []\n        # New property! Keys are the inputs to this node and\n        # their values are the partials of this node with\n        # respect to that input.\n        self.gradients = {}\n        # Sets this node as an outbound node for all of\n        # this node's inputs.\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `forward` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `backward` method.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Input(Node):\n    \"\"\"\n    A generic input into the network.\n    \"\"\"\n    def __init__(self):\n        # The base class constructor has to run to set all\n        # the properties here.\n        #\n        # The most important property on an Input is value.\n        # self.value is set during `topological_sort` later.\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n    def backward(self):\n        # An Input node has no inputs so the gradient (derivative)\n        # is zero.\n        # The key, `self`, is reference to this object.\n        self.gradients = {self: 0}\n        # Weights and bias may be inputs, so you need to sum\n        # the gradient from output gradients.\n        for n in self.outbound_nodes:\n            grad_cost = n.gradients[self]\n            self.gradients[self] += grad_cost * 1\n\n\nclass Linear(Node):\n    \"\"\"\n    Represents a node that performs a linear transform.\n    \"\"\"\n    def __init__(self, X, W, b):\n        # The base class (Node) constructor. Weights and bias\n        # are treated like inbound nodes.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        \"\"\"\n        Performs the math behind a linear transform.\n        \"\"\"\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient based on the output values.\n        \"\"\"\n        # Initialize a partial for each of the inbound_nodes.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Cycle through the outputs. The gradient will change depending\n        # on each output, so the gradients are summed over all outputs.\n        for n in self.outbound_nodes:\n            # Get the partial of the cost with respect to this node.\n            grad_cost = n.gradients[self]\n            # Set the partial of the loss with respect to this node's inputs.\n            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n            # Set the partial of the loss with respect to this node's weights.\n            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n            # Set the partial of the loss with respect to this node's bias.\n            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n\n\nclass Sigmoid(Node):\n    \"\"\"\n    Represents a node that performs the sigmoid activation function.\n    \"\"\"\n    def __init__(self, node):\n        # The base class constructor.\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x))\n\n    def forward(self):\n        \"\"\"\n        Perform the sigmoid function and set the value.\n        \"\"\"\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient using the derivative of\n        the sigmoid function.\n        \"\"\"\n        # Initialize the gradients to 0.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n\n        # Cycle through the outputs. The gradient will change depending\n        # on each output, so the gradients are summed over all outputs.\n        for n in self.outbound_nodes:\n            # Get the partial of the cost with respect to this node.\n            grad_cost = n.gradients[self]\n            \"\"\"\n            TODO: Your code goes here!\n\n            Set the gradients property to the gradients with respect to each input.\n\n            NOTE: See the Linear node and MSE node for examples.\n            \"\"\"\n\n\nclass MSE(Node):\n    def __init__(self, y, a):\n        \"\"\"\n        The mean squared error cost function.\n        Should be used as the last node for a network.\n        \"\"\"\n        # Call the base class' constructor.\n        Node.__init__(self, [y, a])\n\n    def forward(self):\n        \"\"\"\n        Calculates the mean squared error.\n        \"\"\"\n        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n        # errors.\n        #\n        # For example, if we subtract an array of shape (3,) from an array of shape\n        # (3,1) we get an array of shape(3,3) as the result when we want\n        # an array of shape (3,1) instead.\n        #\n        # Making both arrays (3,1) insures the result is (3,1) and does\n        # an elementwise subtraction as expected.\n        y = self.inbound_nodes[0].value.reshape(-1, 1)\n        a = self.inbound_nodes[1].value.reshape(-1, 1)\n\n        self.m = self.inbound_nodes[0].value.shape[0]\n        # Save the computed output for backward.\n        self.diff = y - a\n        self.value = np.mean(self.diff**2)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient of the cost.\n\n        This is the final node of the network so outbound nodes\n        are not a concern.\n        \"\"\"\n        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_and_backward(graph):\n    \"\"\"\n    Performs a forward pass and a backward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `graph`: The result of calling `topological_sort`.\n    \"\"\"\n    # Forward pass\n    for n in graph:\n        n.forward()\n\n    # Backward pass\n    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n    for n in graph[::-1]:\n        n.backward()\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 197542,
          "key": "db18966a-23fa-4810-80b3-9ef1670f2d63",
          "title": "Stochastic Gradient Descent",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 198067,
              "key": "4d8aa383-934c-42a0-b692-0b758d54962f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's my solution to the last quiz.\n\n```\nclass Sigmoid(Node):\n    \"\"\"\n    Represents a node that performs the sigmoid activation function.\n    \"\"\"\n    def __init__(self, node):\n        # The base class constructor.\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x))\n\n    def forward(self):\n        \"\"\"\n        Perform the sigmoid function and set the value.\n        \"\"\"\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient using the derivative of\n        the sigmoid function.\n        \"\"\"\n        # Initialize the gradients to 0.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Sum the derivative with respect to the input over all the outputs.\n        for n in self.outbound_nodes:\n            grad_cost = n.gradients[self]\n            sigmoid = self.value\n            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n```\n\nThe `backward` method sums the derivative (it's a normal derivative when there's only one variable) with respect to the only input over all the output nodes. The last line implements the derivative, <span class=\"mathquill\">\\frac {\\partial sigmoid}{\\partial x} \\frac {\\partial cost}{\\partial sigmoid}</span>.\n\nReplacing the math expression with code:\n\n<span class=\"mathquill\">\\frac {\\partial sigmoid}{\\partial x}</span> is `sigmoid * (1 - sigmoid)` and <span class=\"mathquill\">\\frac {\\partial cost}{\\partial sigmoid}</span> is `grad_cost`.",
              "instructor_notes": ""
            },
            {
              "id": 197894,
              "key": "ebea5ac6-178e-4979-b34f-b6a819da69f1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now that you have the gradient of the cost with respect to each input (the return value from `forward_and_backward()`) your network can start learning! To do so, you will implement a technique called **Stochastic Gradient Descent**.\n\n### Stochastic Gradient Descent\n\nStochastic Gradient Descent (SGD) is a version of Gradient Descent where on each forward pass a batch of data is randomly sampled from total dataset. Remember when we talked about the batch size earlier? That's the size of the batch. Ideally, the entire dataset would be fed into the neural network on each forward pass, but in practice, it's not practical due to memory constraints. SGD is an approximation of Gradient Descent, the more batches processed by the neural network, the better the approximation.\n\nA naïve implementation of SGD involves:\n\n1. Randomly sample a batch of data from the total dataset.\n2. Running the network forward and backward to calculate the gradient (with data from (1)).\n3. Apply the gradient descent update.\n4. Repeat steps 1-3 until convergence or the loop is stopped by another mechanism (i.e. the number of epochs).\n\nIf all goes well, the network's loss should generally trend downwards, indicating more useful weights and biases over time.\n\nSo far, MiniFlow can already do step 2. In the following quiz, steps 1 and 4 are already implemented. It will be your job to implement step 3. \n\nAs a reminder, here's the gradient descent update equation, where <span class=\"mathquill\">\\alpha</span> represents the learning rate:\n\n<span class=\"mathquill\">x = x - \\alpha * \\frac {\\partial cost}{\\partial x}</span>",
              "instructor_notes": ""
            },
            {
              "id": 197897,
              "key": "3d88b502-ebdf-4415-aee9-1f1c0c74637a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We're also going to use an actual dataset for this quiz, the [Boston Housing dataset](https://archive.ics.uci.edu/ml/datasets/Housing). After training the network will be able to predict prices of Boston housing!\n",
              "instructor_notes": ""
            },
            {
              "id": 217314,
              "key": "ca0a16e1-0859-4fdb-b1b3-74168ea32a18",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/58362eae_boston-back-bay-reflection/boston-back-bay-reflection.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ca0a16e1-0859-4fdb-b1b3-74168ea32a18",
              "caption": "Boston's Back Bay\n\nBy Robbie Shade (Flickr: Boston's Back Bay) [CC BY 2.0 (http://creativecommons.org/licenses/by/2.0)], via Wikimedia Commons",
              "alt": null,
              "width": 1280,
              "height": 849,
              "instructor_notes": null
            },
            {
              "id": 216028,
              "key": "c5b24ef5-3bb5-46da-ab5f-002d248a5147",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Each example in the dataset is a description of a house in the Boston suburbs, the description consists of 13 numerical values (features). Each example also has an associated price. With SGD, we're going to minimize the MSE between the actual price and the price predicted by the neural network based on the features.\n\nIf all goes well the output should look something like this:\n\nWhen the batch size is 11:\n\n```sh\nTotal number of examples = 506\nEpoch: 1, Loss: 140.256\nEpoch: 2, Loss: 34.570\nEpoch: 3, Loss: 27.501\nEpoch: 4, Loss: 25.343\nEpoch: 5, Loss: 20.421\nEpoch: 6, Loss: 17.600\nEpoch: 7, Loss: 18.176\nEpoch: 8, Loss: 16.812\nEpoch: 9, Loss: 15.531\nEpoch: 10, Loss: 16.429\n```\n\nWhen the batch size is the same as the total number of examples (batch is the whole dataset):\n\n```sh\nTotal number of examples = 506\nEpoch: 1, Loss: 646.134\nEpoch: 2, Loss: 587.867\nEpoch: 3, Loss: 510.707\nEpoch: 4, Loss: 446.558\nEpoch: 5, Loss: 407.695\nEpoch: 6, Loss: 324.440\nEpoch: 7, Loss: 295.542\nEpoch: 8, Loss: 251.599\nEpoch: 9, Loss: 219.888\nEpoch: 10, Loss: 216.155\n```\n\nNotice the *cost* or *loss* trending towards 0.",
              "instructor_notes": ""
            },
            {
              "id": 197898,
              "key": "b9799b89-77a1-4029-90b8-cca26e80c911",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Instructions\n\n1. Open `nn.py`. See how the network runs with this new architecture.\n2. Find the `sgd_update` method in `miniflow.py` and implement SGD.\n3. Test your network! Does your loss decrease with more epochs?\n\nNote! The virtual machines on which we run your code have time limits. If your network takes more than 10 seconds to run, you will get a timeout error. Keep this in mind as you play with the number of epochs.",
              "instructor_notes": ""
            },
            {
              "id": 197899,
              "key": "a8a6a4b8-9baa-4de4-a3b9-ff7b1a332416",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "Implement SGD",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6382385379803136",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nCheck out the new network architecture and dataset!\n\nNotice that the weights and biases are\ngenerated randomly.\n\nNo need to change anything, but feel free to tweak\nto test your network, play around with the epochs, batch size, etc!\n\"\"\"\n\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.utils import shuffle, resample\nfrom miniflow import *\n\n# Load data\ndata = load_boston()\nX_ = data['data']\ny_ = data['target']\n\n# Normalize data\nX_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n\nn_features = X_.shape[1]\nn_hidden = 10\nW1_ = np.random.randn(n_features, n_hidden)\nb1_ = np.zeros(n_hidden)\nW2_ = np.random.randn(n_hidden, 1)\nb2_ = np.zeros(1)\n\n# Neural network\nX, y = Input(), Input()\nW1, b1 = Input(), Input()\nW2, b2 = Input(), Input()\n\nl1 = Linear(X, W1, b1)\ns1 = Sigmoid(l1)\nl2 = Linear(s1, W2, b2)\ncost = MSE(y, l2)\n\nfeed_dict = {\n    X: X_,\n    y: y_,\n    W1: W1_,\n    b1: b1_,\n    W2: W2_,\n    b2: b2_\n}\n\nepochs = 10\n# Total number of examples\nm = X_.shape[0]\nbatch_size = 11\nsteps_per_epoch = m // batch_size\n\ngraph = topological_sort(feed_dict)\ntrainables = [W1, b1, W2, b2]\n\nprint(\"Total number of examples = {}\".format(m))\n\n# Step 4\nfor i in range(epochs):\n    loss = 0\n    for j in range(steps_per_epoch):\n        # Step 1\n        # Randomly sample a batch of examples\n        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n\n        # Reset value of X and y Inputs\n        X.value = X_batch\n        y.value = y_batch\n\n        # Step 2\n        forward_and_backward(graph)\n\n        # Step 3\n        sgd_update(trainables)\n\n        loss += graph[-1].value\n\n    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "import numpy as np\n\n\nclass Node(object):\n    \"\"\"\n    Base class for nodes in the network.\n\n    Arguments:\n\n        `inbound_nodes`: A list of nodes with edges into this node.\n    \"\"\"\n    def __init__(self, inbound_nodes=[]):\n        \"\"\"\n        Node's constructor (runs when the object is instantiated). Sets\n        properties that all nodes need.\n        \"\"\"\n        # A list of nodes with edges into this node.\n        self.inbound_nodes = inbound_nodes\n        # The eventual value of this node. Set by running\n        # the forward() method.\n        self.value = None\n        # A list of nodes that this node outputs to.\n        self.outbound_nodes = []\n        # New property! Keys are the inputs to this node and\n        # their values are the partials of this node with\n        # respect to that input.\n        self.gradients = {}\n        # Sets this node as an outbound node for all of\n        # this node's inputs.\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `forward` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `backward` method.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Input(Node):\n    \"\"\"\n    A generic input into the network.\n    \"\"\"\n    def __init__(self):\n        # The base class constructor has to run to set all\n        # the properties here.\n        #\n        # The most important property on an Input is value.\n        # self.value is set during `topological_sort` later.\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n    def backward(self):\n        # An Input node has no inputs so the gradient (derivative)\n        # is zero.\n        # The key, `self`, is reference to this object.\n        self.gradients = {self: 0}\n        # Weights and bias may be inputs, so you need to sum\n        # the gradient from output gradients.\n        for n in self.outbound_nodes:\n            self.gradients[self] += n.gradients[self]\n\nclass Linear(Node):\n    \"\"\"\n    Represents a node that performs a linear transform.\n    \"\"\"\n    def __init__(self, X, W, b):\n        # The base class (Node) constructor. Weights and bias\n        # are treated like inbound nodes.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        \"\"\"\n        Performs the math behind a linear transform.\n        \"\"\"\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient based on the output values.\n        \"\"\"\n        # Initialize a partial for each of the inbound_nodes.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Cycle through the outputs. The gradient will change depending\n        # on each output, so the gradients are summed over all outputs.\n        for n in self.outbound_nodes:\n            # Get the partial of the cost with respect to this node.\n            grad_cost = n.gradients[self]\n            # Set the partial of the loss with respect to this node's inputs.\n            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n            # Set the partial of the loss with respect to this node's weights.\n            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n            # Set the partial of the loss with respect to this node's bias.\n            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n\n\nclass Sigmoid(Node):\n    \"\"\"\n    Represents a node that performs the sigmoid activation function.\n    \"\"\"\n    def __init__(self, node):\n        # The base class constructor.\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x))\n\n    def forward(self):\n        \"\"\"\n        Perform the sigmoid function and set the value.\n        \"\"\"\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient using the derivative of\n        the sigmoid function.\n        \"\"\"\n        # Initialize the gradients to 0.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Sum the partial with respect to the input over all the outputs.\n        for n in self.outbound_nodes:\n            grad_cost = n.gradients[self]\n            sigmoid = self.value\n            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n\n\nclass MSE(Node):\n    def __init__(self, y, a):\n        \"\"\"\n        The mean squared error cost function.\n        Should be used as the last node for a network.\n        \"\"\"\n        # Call the base class' constructor.\n        Node.__init__(self, [y, a])\n\n    def forward(self):\n        \"\"\"\n        Calculates the mean squared error.\n        \"\"\"\n        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n        # errors.\n        #\n        # For example, if we subtract an array of shape (3,) from an array of shape\n        # (3,1) we get an array of shape(3,3) as the result when we want\n        # an array of shape (3,1) instead.\n        #\n        # Making both arrays (3,1) insures the result is (3,1) and does\n        # an elementwise subtraction as expected.\n        y = self.inbound_nodes[0].value.reshape(-1, 1)\n        a = self.inbound_nodes[1].value.reshape(-1, 1)\n\n        self.m = self.inbound_nodes[0].value.shape[0]\n        # Save the computed output for backward.\n        self.diff = y - a\n        self.value = np.mean(self.diff**2)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient of the cost.\n        \"\"\"\n        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_and_backward(graph):\n    \"\"\"\n    Performs a forward pass and a backward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `graph`: The result of calling `topological_sort`.\n    \"\"\"\n    # Forward pass\n    for n in graph:\n        n.forward()\n\n    # Backward pass\n    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n    for n in graph[::-1]:\n        n.backward()\n\n\ndef sgd_update(trainables, learning_rate=1e-2):\n    \"\"\"\n    Updates the value of each trainable with SGD.\n\n    Arguments:\n\n        `trainables`: A list of `Input` Nodes representing weights/biases.\n        `learning_rate`: The learning rate.\n    \"\"\"\n    # TODO: update all the `trainables` with SGD\n    # You can access and assign the value of a trainable with `value` attribute.\n    # Example:\n    # for t in trainables:\n    #   t.value = your implementation here\n    pass\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 198066,
          "key": "84bbf916-4cf5-4354-9d0a-eacc59802488",
          "title": "SGD Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 198068,
              "key": "61013e3b-fbde-4703-970f-9517b27a6d09",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here's my solution to the last quiz.\n\n```python\ndef sgd_update(trainables, learning_rate=1e-2):\n    \"\"\"\n    Updates the value of each trainable with SGD.\n\n    Arguments:\n\n        `trainables`: A list of `Input` nodes representing weights/biases.\n        `learning_rate`: The learning rate.\n    \"\"\"\n    # Performs SGD\n    #\n    # Loop over the trainables\n    for t in trainables:\n        # Change the trainable's value by subtracting the learning rate\n        # multiplied by the partial of the cost with respect to this\n        # trainable.\n        partial = t.gradients[t]\n        t.value -= learning_rate * partial\n\n```",
              "instructor_notes": ""
            },
            {
              "id": 198071,
              "key": "9f7521e8-0bf2-47c4-8470-02622002cda8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Take a look at the last few lines:\n\n```\n# Performs SGD\n#\n# Loop over the trainables\nfor t in trainables:\n    # Change the trainable's value by subtracting the learning rate\n    # multiplied by the partial of the cost with respect to this\n    # trainable.\n    partial = t.gradients[t]\n    t.value -= learning_rate * partial\n\n```\n\nThere are two keys steps. First, the partial of the cost (*C*) with respect to the trainable `t` is accessed.\n\n```\npartial = t.gradients[t]\n```\n\nSecond, the value of the trainable is updated according to Equation (12).\n\n```\nt.value -= learning_rate * partial\n```\n\nThis is done for all trainables.",
              "instructor_notes": ""
            },
            {
              "id": 198073,
              "key": "5f05f125-cd20-430f-b1d0-eea17aa9e56b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/581021c4_12/12.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5f05f125-cd20-430f-b1d0-eea17aa9e56b",
              "caption": "Equation (12)",
              "alt": null,
              "width": 125,
              "height": 21,
              "instructor_notes": null
            },
            {
              "id": 198074,
              "key": "8e9db25b-ba29-489d-9e29-37c2d234022c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nWith that, the loss decreases on the next pass through the network.\n\nI'm putting the same quiz below again. If you haven't already, set the number of epochs to something like `1000` and watch as the loss decreases!",
              "instructor_notes": ""
            },
            {
              "id": 198075,
              "key": "6b658590-9b17-417f-a0fd-59db0e20b71e",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5861710789476352",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nHave fun with the number of epochs!\n\nBe warned that if you increase them too much,\nthe VM will time out :)\n\"\"\"\n\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.utils import shuffle, resample\nfrom miniflow import *\n\n# Load data\ndata = load_boston()\nX_ = data['data']\ny_ = data['target']\n\n# Normalize data\nX_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n\nn_features = X_.shape[1]\nn_hidden = 10\nW1_ = np.random.randn(n_features, n_hidden)\nb1_ = np.zeros(n_hidden)\nW2_ = np.random.randn(n_hidden, 1)\nb2_ = np.zeros(1)\n\n# Neural network\nX, y = Input(), Input()\nW1, b1 = Input(), Input()\nW2, b2 = Input(), Input()\n\nl1 = Linear(X, W1, b1)\ns1 = Sigmoid(l1)\nl2 = Linear(s1, W2, b2)\ncost = MSE(y, l2)\n\nfeed_dict = {\n    X: X_,\n    y: y_,\n    W1: W1_,\n    b1: b1_,\n    W2: W2_,\n    b2: b2_\n}\n\nepochs = 10\n# Total number of examples\nm = X_.shape[0]\nbatch_size = 11\nsteps_per_epoch = m // batch_size\n\ngraph = topological_sort(feed_dict)\ntrainables = [W1, b1, W2, b2]\n\nprint(\"Total number of examples = {}\".format(m))\n\n# Step 4\nfor i in range(epochs):\n    loss = 0\n    for j in range(steps_per_epoch):\n        # Step 1\n        # Randomly sample a batch of examples\n        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n\n        # Reset value of X and y Inputs\n        X.value = X_batch\n        y.value = y_batch\n\n        # Step 2\n        forward_and_backward(graph)\n\n        # Step 3\n        sgd_update(trainables)\n\n        loss += graph[-1].value\n\n    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
                    "name": "nn.py"
                  },
                  {
                    "text": "import numpy as np\n\nclass Node:\n    \"\"\"\n    Base class for nodes in the network.\n\n    Arguments:\n\n        `inbound_nodes`: A list of nodes with edges into this node.\n    \"\"\"\n    def __init__(self, inbound_nodes=[]):\n        \"\"\"\n        Node's constructor (runs when the object is instantiated). Sets\n        properties that all nodes need.\n        \"\"\"\n        # A list of nodes with edges into this node.\n        self.inbound_nodes = inbound_nodes\n        # The eventual value of this node. Set by running\n        # the forward() method.\n        self.value = None\n        # A list of nodes that this node outputs to.\n        self.outbound_nodes = []\n        # New property! Keys are the inputs to this node and\n        # their values are the partials of this node with\n        # respect to that input.\n        self.gradients = {}\n        # Sets this node as an outbound node for all of\n        # this node's inputs.\n        for node in inbound_nodes:\n            node.outbound_nodes.append(self)\n\n    def forward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `forward` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self):\n        \"\"\"\n        Every node that uses this class as a base class will\n        need to define its own `backward` method.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Input(Node):\n    \"\"\"\n    A generic input into the network.\n    \"\"\"\n    def __init__(self):\n        # The base class constructor has to run to set all\n        # the properties here.\n        #\n        # The most important property on an Input is value.\n        # self.value is set during `topological_sort` later.\n        Node.__init__(self)\n\n    def forward(self):\n        # Do nothing because nothing is calculated.\n        pass\n\n    def backward(self):\n        # An Input node has no inputs so the gradient (derivative)\n        # is zero.\n        # The key, `self`, is reference to this object.\n        self.gradients = {self: 0}\n        # Weights and bias may be inputs, so you need to sum\n        # the gradient from output gradients.\n        for n in self.outbound_nodes:\n            self.gradients[self] += n.gradients[self]\n\nclass Linear(Node):\n    \"\"\"\n    Represents a node that performs a linear transform.\n    \"\"\"\n    def __init__(self, X, W, b):\n        # The base class (Node) constructor. Weights and bias\n        # are treated like inbound nodes.\n        Node.__init__(self, [X, W, b])\n\n    def forward(self):\n        \"\"\"\n        Performs the math behind a linear transform.\n        \"\"\"\n        X = self.inbound_nodes[0].value\n        W = self.inbound_nodes[1].value\n        b = self.inbound_nodes[2].value\n        self.value = np.dot(X, W) + b\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient based on the output values.\n        \"\"\"\n        # Initialize a partial for each of the inbound_nodes.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Cycle through the outputs. The gradient will change depending\n        # on each output, so the gradients are summed over all outputs.\n        for n in self.outbound_nodes:\n            # Get the partial of the cost with respect to this node.\n            grad_cost = n.gradients[self]\n            # Set the partial of the loss with respect to this node's inputs.\n            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n            # Set the partial of the loss with respect to this node's weights.\n            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n            # Set the partial of the loss with respect to this node's bias.\n            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n\n\nclass Sigmoid(Node):\n    \"\"\"\n    Represents a node that performs the sigmoid activation function.\n    \"\"\"\n    def __init__(self, node):\n        # The base class constructor.\n        Node.__init__(self, [node])\n\n    def _sigmoid(self, x):\n        \"\"\"\n        This method is separate from `forward` because it\n        will be used with `backward` as well.\n\n        `x`: A numpy array-like object.\n        \"\"\"\n        return 1. / (1. + np.exp(-x))\n\n    def forward(self):\n        \"\"\"\n        Perform the sigmoid function and set the value.\n        \"\"\"\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient using the derivative of\n        the sigmoid function.\n        \"\"\"\n        # Initialize the gradients to 0.\n        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n        # Sum the partial with respect to the input over all the outputs.\n        for n in self.outbound_nodes:\n            grad_cost = n.gradients[self]\n            sigmoid = self.value\n            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n\n\nclass MSE(Node):\n    def __init__(self, y, a):\n        \"\"\"\n        The mean squared error cost function.\n        Should be used as the last node for a network.\n        \"\"\"\n        # Call the base class' constructor.\n        Node.__init__(self, [y, a])\n\n    def forward(self):\n        \"\"\"\n        Calculates the mean squared error.\n        \"\"\"\n        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n        # errors.\n        #\n        # For example, if we subtract an array of shape (3,) from an array of shape\n        # (3,1) we get an array of shape(3,3) as the result when we want\n        # an array of shape (3,1) instead.\n        #\n        # Making both arrays (3,1) insures the result is (3,1) and does\n        # an elementwise subtraction as expected.\n        y = self.inbound_nodes[0].value.reshape(-1, 1)\n        a = self.inbound_nodes[1].value.reshape(-1, 1)\n\n        self.m = self.inbound_nodes[0].value.shape[0]\n        # Save the computed output for backward.\n        self.diff = y - a\n        self.value = np.mean(self.diff**2)\n\n    def backward(self):\n        \"\"\"\n        Calculates the gradient of the cost.\n        \"\"\"\n        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n\n\ndef topological_sort(feed_dict):\n    \"\"\"\n    Sort the nodes in topological order using Kahn's Algorithm.\n\n    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n\n    Returns a list of sorted nodes.\n    \"\"\"\n\n    input_nodes = [n for n in feed_dict.keys()]\n\n    G = {}\n    nodes = [n for n in input_nodes]\n    while len(nodes) > 0:\n        n = nodes.pop(0)\n        if n not in G:\n            G[n] = {'in': set(), 'out': set()}\n        for m in n.outbound_nodes:\n            if m not in G:\n                G[m] = {'in': set(), 'out': set()}\n            G[n]['out'].add(m)\n            G[m]['in'].add(n)\n            nodes.append(m)\n\n    L = []\n    S = set(input_nodes)\n    while len(S) > 0:\n        n = S.pop()\n\n        if isinstance(n, Input):\n            n.value = feed_dict[n]\n\n        L.append(n)\n        for m in n.outbound_nodes:\n            G[n]['out'].remove(m)\n            G[m]['in'].remove(n)\n            # if no other incoming edges add to S\n            if len(G[m]['in']) == 0:\n                S.add(m)\n    return L\n\n\ndef forward_and_backward(graph):\n    \"\"\"\n    Performs a forward pass and a backward pass through a list of sorted Nodes.\n\n    Arguments:\n\n        `graph`: The result of calling `topological_sort`.\n    \"\"\"\n    # Forward pass\n    for n in graph:\n        n.forward()\n\n    # Backward pass\n    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n    for n in graph[::-1]:\n        n.backward()\n\n\ndef sgd_update(trainables, learning_rate=1e-2):\n    \"\"\"\n    Updates the value of each trainable with SGD.\n\n    Arguments:\n\n        `trainables`: A list of `Input` Nodes representing weights/biases.\n        `learning_rate`: The learning rate.\n    \"\"\"\n    # Performs SGD\n    #\n    # Loop over the trainables\n    for t in trainables:\n        # Change the trainable's value by subtracting the learning rate\n        # multiplied by the partial of the cost with respect to this\n        # trainable.\n        partial = t.gradients[t]\n        t.value -= learning_rate * partial\n",
                    "name": "miniflow.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 262861,
          "key": "06efc158-6f7e-414a-b35b-0022bbd04ac8",
          "title": "Outro",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 262863,
              "key": "d796061d-ac19-4775-8d0d-0f55a73a2519",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/October/58104033_tensorflow-825x510/tensorflow-825x510.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d796061d-ac19-4775-8d0d-0f55a73a2519",
              "caption": "",
              "alt": null,
              "width": 825,
              "height": 510,
              "instructor_notes": null
            },
            {
              "id": 262865,
              "key": "bda891ef-88bf-4d6a-8856-1b37426ca5df",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Congratulations on making it to the end of this lab! Building a neural network from scratch is no small task. You should be proud!\n\nMiniFlow has the makings of becoming a powerful deep learning tool. It is entirely possible to classify something like the [MNIST](http://yann.lecun.com/exdb/mnist/) database with MiniFlow.\n\nI'll leave it as an exercise for you to finish MiniFlow from here.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}