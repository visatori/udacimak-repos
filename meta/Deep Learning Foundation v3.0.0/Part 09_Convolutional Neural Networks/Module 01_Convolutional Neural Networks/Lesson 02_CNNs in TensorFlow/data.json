{
  "data": {
    "lesson": {
      "id": 419661,
      "key": "9ee528ea-650c-4844-8b0a-eb63ce83103c",
      "title": "CNNs in TensorFlow",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": null,
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": null,
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 420872,
          "key": "91ce195e-5abc-4a50-8a35-39b50a45b506",
          "title": "Convolutional Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420899,
              "key": "c99cab5b-ebdb-4aeb-b995-b354042bdae4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Convolutional Layers\n\nThe image below is an example of a [convolution](https://en.wikipedia.org/wiki/Convolution) with a 3x3 filter and a stride of 1.",
              "instructor_notes": ""
            },
            {
              "id": 420900,
              "key": "c7e23189-8f25-4ae3-ba09-7c419ebd281d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/May/5913f852_convolution-schematic/convolution-schematic.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c7e23189-8f25-4ae3-ba09-7c419ebd281d",
              "caption": "Convolution with 3Ã—3 Filter.  Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution",
              "alt": "",
              "width": 263,
              "height": 192,
              "instructor_notes": null
            },
            {
              "id": 420901,
              "key": "b4cbbc98-6c62-45be-8880-37f4a0699249",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The convolution for each 3x3 section is calculated against the weight, `[[1, 0, 1], [0, 1, 0], [1, 0, 1]]`, and then a bias is added to create the convolved feature on the right.  In this case, the bias is zero.  ",
              "instructor_notes": ""
            },
            {
              "id": 420881,
              "key": "b8a71df0-b2f5-4329-9616-0b2b982e3b86",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Convolutional Layers in TensorFlow\n\nLet's examine how to implement a convolutional layer in TensorFlow.\n\nTensorFlow provides the [`tf.nn.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d), [`tf.nn.bias_add()`](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add), and [`tf.nn.relu()`](https://www.tensorflow.org/api_docs/python/tf/nn/relu) functions to create your own convolutional layers.\n```python\n# output depth\nk_output = 64\n\n# image dimensions\nimage_width = 10\nimage_height = 10\ncolor_channels = 3\n\n# convolution filter dimensions\nfilter_size_width = 5\nfilter_size_height = 5\n\n# input/image\ninput = tf.placeholder(\n    tf.float32,\n    shape=[None, image_height, image_width, color_channels])\n\n# weight and bias\nweight = tf.Variable(tf.truncated_normal(\n    [filter_size_height, filter_size_width, color_channels, k_output]))\nbias = tf.Variable(tf.zeros(k_output))\n\n# apply convolution\nconv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n# add bias\nconv_layer = tf.nn.bias_add(conv_layer, bias)\n# apply activation function\nconv_layer = tf.nn.relu(conv_layer)\n```\nThe code above uses the [`tf.nn.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) function to compute the convolution with `weight` as the filter and `[1, 2, 2, 1]` for the strides.  \n- TensorFlow uses a stride for each `input` dimension, `[batch, input_height, input_width, input_channels]`.\n- We generally always set the stride for `batch` and `input_channels` (i.e. the first and fourth element in the `strides` array) to be `1`.  This ensures that the model uses all batches and input channels.  (_It's good practice to remove the batches or channels you want to skip from the data set rather than use a stride to skip them._)  \n- You'll focus on changing `input_height` and  `input_width` (while setting `batch` and `input_channels` to 1).  The `input_height` and `input_width` strides are for striding the filter over `input`. This example code uses a stride of 2 with 5x5 filter over `input`.  I've mentioned stride as one number because you usually have a square stride where `height = width`.  When someone says they are using a stride of 2, they usually mean `tf.nn.conv2d(x, W, strides=[1, 2, 2, 1])`.\n\nThe [`tf.nn.bias_add()`](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add) function adds a 1-d bias to the last dimension in a matrix.  (**Note: using [`tf.add()`](https://www.tensorflow.org/api_docs/python/tf/add) doesn't work when the tensors aren't the same shape.**)\n\nThe [`tf.nn.relu()`](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function applies a ReLU activation function to the layer.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420873,
          "key": "e3d07b8c-15e2-4865-bddf-b7f773eadc06",
          "title": "Quiz: Convolutional Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420875,
              "key": "b7a486fc-0448-438b-8d18-f11f2b0cfdde",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Using Convolutional Layers in TensorFlow\n\nLet's now build a convolutional layer in TensorFlow. In the below exercise, you'll be asked to set up the dimensions of the convolution filters, the weights, and the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straightforward.\n\n### Review\n\nYou should go over the TensorFlow documentation for [2D convolutions](https://www.tensorflow.org/api_guides/python/nn#Convolution). Most of the documentation is straightforward, except perhaps the `padding` argument. The padding might differ depending on whether you pass `'VALID'` or `'SAME'`.\n\nHere are a few more things worth reviewing:\n\n1. Introduction to TensorFlow -> [TensorFlow Variables](https://www.tensorflow.org/programmers_guide/variables).\n2. How to determine the dimensions of the output based on the input size and the filter size (shown below). You'll use this to determine what the size of your filter should be. \n   ```\n    new_height = (input_height - filter_height + 2 * P)/S + 1\n    new_width = (input_width - filter_width + 2 * P)/S + 1\n    ```\n\n### Instructions\n\n1. Finish off each `TODO` in the `conv2d` function.\n2. Set up the `strides`, `padding`, filter weight (`F_w`), and filter bias (`F_b`) such that\nthe output shape is `(1, 2, 2, 3)`. Note that all of these except `strides` should be TensorFlow variables.",
              "instructor_notes": ""
            },
            {
              "id": 420876,
              "key": "a98d3109-64b7-4466-9271-e055d884eb07",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "4850930108465152",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nSetup the strides, padding and filter weight/bias such that\nthe output shape is (1, 2, 2, 3).\n\"\"\"\nimport tensorflow as tf\nimport numpy as np\n\n# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n# (1, 4, 4, 1)\nx = np.array([\n    [0, 1, 0.5, 10],\n    [2, 2.5, 1, -8],\n    [4, 0, 5, 6],\n    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\nX = tf.constant(x)\n\n\ndef conv2d(input):\n    # Filter (weights and bias)\n    # The shape of the filter weight is (height, width, input_depth, output_depth)\n    # The shape of the filter bias is (output_depth,)\n    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n    F_W = ?\n    F_b = ?\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n    strides = [?, ?, ?, ?]\n    # TODO: set the padding, either 'VALID' or 'SAME'.\n    padding = ?\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n\nout = conv2d(X)\n\n\n\n",
                    "name": "conv.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 420874,
          "key": "aca43c3a-52d0-4758-bbac-e696a82442db",
          "title": "Solution: Convolutional Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420878,
              "key": "33458da5-ca9d-4ebd-8ab5-c03b170a1b86",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Solution\n\nHere's how I did it. **NOTE**: there is more than one way to get the correct output shape. Your answer might differ from mine.",
              "instructor_notes": ""
            },
            {
              "id": 420879,
              "key": "4923bf86-67dc-42ba-96d5-b8652dd416c6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "```python\ndef conv2d(input):\n    # create the filter (weights and bias)\n    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n    F_b = tf.Variable(tf.zeros(3))\n    strides = [1, 2, 2, 1]\n    padding = 'VALID'\n    x = tf.nn.conv2d(input, F_W, strides, padding)\n    return tf.nn.bias_add(x, F_b)\n```\n    ",
              "instructor_notes": ""
            },
            {
              "id": 420880,
              "key": "46dfce17-1cdf-4655-8b58-ebdecec2d4a2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 3)`. I choose `'VALID'` for the padding algorithm. I find it simpler to understand, and it achieves the result I'm looking for.\n\n```sh\nout_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\nout_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n```\n\nPlugging in the values:\n\n```sh\nout_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\nout_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n```\n\nIn order to change the depth from 1 to 3, I have to set the output depth of my filter appropriately:\n\n```python\nF_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth)\nF_b = tf.Variable(tf.zeros(3)) # (output_depth)\n```\nThe input has a depth of 1, so I set that as the `input_depth` of the filter.\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420882,
          "key": "0439bbf0-1953-4e43-99a1-87e34a1718fe",
          "title": "Max Pooling Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420893,
              "key": "0edb21c5-464f-4f79-9c59-44f20442ee31",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Max Pooling Layers in TensorFlow\n\nThe image below is an example of [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) with a 2x2 filter and stride of 2.  The four 2x2 colors represent each time the filter was applied to find the maximum value.  ",
              "instructor_notes": ""
            },
            {
              "id": 420885,
              "key": "68a0e0bd-5ab7-423c-952d-7e6e0deffc52",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2016/November/582aac09_max-pooling/max-pooling.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/68a0e0bd-5ab7-423c-952d-7e6e0deffc52",
              "caption": "By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons",
              "alt": "",
              "width": 570,
              "height": 330,
              "instructor_notes": null
            },
            {
              "id": 420886,
              "key": "fcbb6c3f-2930-4dde-94b4-7866ad81e9b6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For example, `[[1, 0], [4, 6]]` becomes `6`, because `6` is the maximum value in this set. Similarly, `[[2, 3], [6, 8]]` becomes `8`.\n\nConceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.\n\nTensorFlow provides the  [`tf.nn.max_pool()`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function to apply [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) to your convolutional layers.\n```python\n...\nconv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\nconv_layer = tf.nn.bias_add(conv_layer, bias)\nconv_layer = tf.nn.relu(conv_layer)\n# apply max pooling\nconv_layer = tf.nn.max_pool(\n    conv_layer,\n    ksize=[1, 2, 2, 1],\n    strides=[1, 2, 2, 1],\n    padding='SAME')\n```\nThe [`tf.nn.max_pool()`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function performs max pooling with the `ksize` parameter as the size of the filter and the `strides` parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n\nThe `ksize` and `strides` parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor (`[batch, height, width, channels]`). For both `ksize` and `strides`, the batch and channel dimensions are typically set to `1`.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420883,
          "key": "feb20a92-d66a-44f6-b116-7f06d8b97e31",
          "title": "Quiz: Max Pooling Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420887,
              "key": "4108a4d6-7c16-4324-aaf4-349613db1b48",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Using Max Pooling Layers in TensorFlow\n\nIn the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. \n\n### Review\n\nYou should go over the TensorFlow documentation for [`tf.nn.max_pool()`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool). Padding works the same as it does for a convolution.\n\n\n### Instructions\n\n1. Finish off each `TODO` in the `maxpool` function.\n\n2. Setup the `strides`, `padding` and `ksize` such that the output shape after pooling is `(1, 2, 2, 1)`.",
              "instructor_notes": ""
            },
            {
              "id": 420888,
              "key": "01342cf3-6dfe-4eab-a31f-b664f6f7b396",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "5823098908704768",
                "initial_code_files": [
                  {
                    "text": "\"\"\"\nSet the values to `strides` and `ksize` such that\nthe output shape after pooling is (1, 2, 2, 1).\n\"\"\"\nimport tensorflow as tf\nimport numpy as np\n\n# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n# (1, 4, 4, 1)\nx = np.array([\n    [0, 1, 0.5, 10],\n    [2, 2.5, 1, -8],\n    [4, 0, 5, 6],\n    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\nX = tf.constant(x)\n\ndef maxpool(input):\n    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n    ksize = [?, ?, ?, ?]\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n    strides = [?, ?, ?, ?]\n    # TODO: set the padding, either 'VALID' or 'SAME'.\n    padding = ?\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n    return tf.nn.max_pool(input, ksize, strides, padding)\n    \nout = maxpool(X)",
                    "name": "pool.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 420884,
          "key": "8b893a2e-d636-4ff5-afb0-1dce05260082",
          "title": "Solution: Max Pooling Layers",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420890,
              "key": "d04f1b2f-294e-4800-8ced-45d26c18bbea",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Solution\n\nHere's how I did it. **NOTE**: there is more than one way to get the correct output shape. Your answer might differ from mine.",
              "instructor_notes": ""
            },
            {
              "id": 420891,
              "key": "b1aec2fa-05a1-4afb-b9ae-f01d9dd3c511",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "```python\ndef maxpool(input):\n    ksize = [1, 2, 2, 1]\n    strides = [1, 2, 2, 1]\n    padding = 'VALID'\n    return tf.nn.max_pool(input, ksize, strides, padding)\n```",
              "instructor_notes": ""
            },
            {
              "id": 420892,
              "key": "4b69550e-e444-4b05-8bac-11b3e08d8e9c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 1)`. I choose `'VALID'` for the padding algorithm. I find it simpler to understand and it achieves the result I'm looking for.\n\n```sh\nout_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\nout_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n```\n\nPlugging in the values:\n\n```sh\nout_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\nout_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n```\n\nThe depth doesn't change during a pooling operation so I don't have to worry about that.\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420895,
          "key": "bf7fee5b-e5dd-47ac-80ec-5017daaaf2b9",
          "title": "CNNs in TensorFlow",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 420903,
              "key": "e3809829-1176-4bfc-995d-fcef798ef45a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Convolutional Neural Networks in TensorFlow\nIt's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow. ",
              "instructor_notes": ""
            },
            {
              "id": 420902,
              "key": "ff3b7658-0bdf-498b-bd9d-740dd1423486",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59db9ea8_arch/arch.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ff3b7658-0bdf-498b-bd9d-740dd1423486",
              "caption": "Example CNN Architecture",
              "alt": "",
              "width": 2594,
              "height": 1312,
              "instructor_notes": null
            },
            {
              "id": 420896,
              "key": "92b93728-b16b-421e-9b04-53510162b3d9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n\nThe code you'll be looking at is similar to what you saw in the segment on **Tensorflow** in the **Neural Networks** lesson, except we restructured the architecture of this network as a CNN. \n\nJust like in that segment, here you'll study the line-by-line breakdown of the code. If you want, you can even [download the code](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61ca1_cnn/cnn.zip) and run it yourself.\n\nThanks to [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples) for providing the original TensorFlow model on which this segment is based.\n\nTime to dive in!\n\n### Dataset\nYou've seen this section of code from previous lessons.  Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and one-hot encode the data.\n```python\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n\nimport tensorflow as tf\n\n# parameters\nlearning_rate = 0.00001\nepochs = 10\nbatch_size = 128\n\n# number of samples to calculate validation and accuracy\n# decrease this if you're running out of memory\ntest_valid_size = 256\n\n# network Parameters\nn_classes = 10  # MNIST total classes (0-9 digits)\ndropout = 0.75  # dropout (probability to keep units)\n```\n\n### Weights and Biases\n\nIn the code below, we will create 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer.  We begin by defining the necessary weights and biases.\n\n```python\n# store weights & biases\nweights = {\n    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'out': tf.Variable(tf.random_normal([n_classes]))}\n```\n\n### Convolutional Layers\n\nRecall that TensorFlow provides the [`tf.nn.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d), [`tf.nn.bias_add()`](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add), and [`tf.nn.relu()`](https://www.tensorflow.org/api_docs/python/tf/nn/relu) functions to create your own convolutional layers.  To simplify our code, we define a useful function below.\n\n```python\ndef conv2d(x, W, b, strides=1):\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n```\n\nThe `conv2d` function computes the convolution against weight `W`, and then adds bias `b`, before applying a ReLU activation function.\n\n### Max Pooling Layers\n\nRecall that TensorFlow provides the  [`tf.nn.max_pool()`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function to apply [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) to your convolutional layers.  To simplify our code, we define a useful function below.\n\n```python\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(\n        x,\n        ksize=[1, k, k, 1],\n        strides=[1, k, k, 1],\n        padding='SAME')\n```\n\nThe `maxpool2d` function applies max pooling to layer `x` using a filter of size `k`.\n\n### Model\n\nThe transformation of each layer to new dimensions is shown in the comments.  For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step.  The next step applies max pooling, turning each sample into 14x14x32.  All the layers are applied from `conv1` to `output`, producing 10 class predictions.\n```python\ndef conv_net(x, weights, biases, dropout):\n    # Layer 1 - 28*28*1 to 14*14*32\n    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Layer 2 - 14*14*32 to 7*7*64\n    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n    conv2 = maxpool2d(conv2, k=2)\n\n    # Fully connected layer - 7*7*64 to 1024\n    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    # Output Layer - class prediction - 1024 to 10\n    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    return out\n```\n\n### Session\nNow let's run it!\n```python\n# tf Graph input\nx = tf.placeholder(tf.float32, [None, 28, 28, 1])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32)\n\n# Model\nlogits = conv_net(x, weights, biases, keep_prob)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(\\\n    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n    .minimize(cost)\n\n# Accuracy\ncorrect_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf. global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(epochs):\n        for batch in range(mnist.train.num_examples//batch_size):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            sess.run(optimizer, feed_dict={\n                x: batch_x,\n                y: batch_y,\n                keep_prob: dropout})\n\n            # Calculate batch loss and accuracy\n            loss = sess.run(cost, feed_dict={\n                x: batch_x,\n                y: batch_y,\n                keep_prob: 1.})\n            valid_acc = sess.run(accuracy, feed_dict={\n                x: mnist.validation.images[:test_valid_size],\n                y: mnist.validation.labels[:test_valid_size],\n                keep_prob: 1.})\n\n            print('Epoch {:>2}, Batch {:>3} -'\n                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n                epoch + 1,\n                batch + 1,\n                loss,\n                valid_acc))\n\n    # Calculate Test Accuracy\n    test_acc = sess.run(accuracy, feed_dict={\n        x: mnist.test.images[:test_valid_size],\n        y: mnist.test.labels[:test_valid_size],\n        keep_prob: 1.})\n    print('Testing Accuracy: {}'.format(test_acc))\n```\nThat's it!  That is a CNN in TensorFlow. ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 216782,
          "key": "01a36ddb-1e9a-47db-95b7-d0093aed970d",
          "title": "CNNs - Additional Resources",
          "semantic_type": "Concept",
          "is_public": true,
          "resources": null,
          "atoms": [
            {
              "id": 216783,
              "key": "629f2f90-c4ef-4a93-a877-05185577fd6d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Additional Resources\n\nThere are many wonderful free resources that allow you to go into more depth around Convolutional Neural Networks. In this course, our goal is to give you just enough intuition to start applying this concept on real world problems so you have enough of an exposure to explore more on your own. We strongly encourage you to explore some of these resources more to reinforce your intuition and explore different ideas.\n\nThese are the resources we recommend in particular:\n\n- Andrej Karpathy's [CS231n Stanford course](http://cs231n.github.io/) on Convolutional Neural Networks.\n- Michael Nielsen's [free book](http://neuralnetworksanddeeplearning.com) on Deep Learning.\n- Goodfellow, Bengio, and Courville's more advanced [free book](http://deeplearningbook.org/) on Deep Learning.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}